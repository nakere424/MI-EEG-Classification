{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DgIW4bhmSVZ3",
        "outputId": "d400e6ad-bd5c-4bd1-d94c-218dd9e7db07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: tensorflow 2.8.0\n",
            "Uninstalling tensorflow-2.8.0:\n",
            "  Would remove:\n",
            "    /usr/local/bin/estimator_ckpt_converter\n",
            "    /usr/local/bin/import_pb_to_tensorboard\n",
            "    /usr/local/bin/saved_model_cli\n",
            "    /usr/local/bin/tensorboard\n",
            "    /usr/local/bin/tf_upgrade_v2\n",
            "    /usr/local/bin/tflite_convert\n",
            "    /usr/local/bin/toco\n",
            "    /usr/local/bin/toco_from_protos\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow-2.8.0.dist-info/*\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow/*\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled tensorflow-2.8.0\n",
            "Collecting tensorflow==2.2.0\n",
            "  Downloading tensorflow-2.2.0-cp37-cp37m-manylinux2010_x86_64.whl (516.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 516.2 MB 3.7 kB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.1.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.0.0)\n",
            "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.4.1)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.14.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (3.17.3)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.44.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (0.37.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (0.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.15.0)\n",
            "Collecting gast==0.3.3\n",
            "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.1.2)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.21.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (3.3.0)\n",
            "Collecting tensorboard<2.3.0,>=2.2.0\n",
            "  Downloading tensorboard-2.2.2-py3-none-any.whl (3.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0 MB 37.2 MB/s \n",
            "\u001b[?25hCollecting h5py<2.11.0,>=2.10.0\n",
            "  Downloading h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 34.1 MB/s \n",
            "\u001b[?25hCollecting tensorflow-estimator<2.3.0,>=2.2.0\n",
            "  Downloading tensorflow_estimator-2.2.0-py2.py3-none-any.whl (454 kB)\n",
            "\u001b[K     |████████████████████████████████| 454 kB 54.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.6.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (0.4.6)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.35.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (2.23.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (57.4.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.0.1)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.10.0.2)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (2021.10.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.2.0)\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, h5py, gast, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.8.0\n",
            "    Uninstalling tensorflow-estimator-2.8.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.8.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.8.0\n",
            "    Uninstalling tensorboard-2.8.0:\n",
            "      Successfully uninstalled tensorboard-2.8.0\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.1.0\n",
            "    Uninstalling h5py-3.1.0:\n",
            "      Successfully uninstalled h5py-3.1.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.5.3\n",
            "    Uninstalling gast-0.5.3:\n",
            "      Successfully uninstalled gast-0.5.3\n",
            "Successfully installed gast-0.3.3 h5py-2.10.0 tensorboard-2.2.2 tensorflow-2.2.0 tensorflow-estimator-2.2.0\n",
            "Collecting tensorflow-addons==0.9.1\n",
            "  Downloading tensorflow_addons-0.9.1-cp37-cp37m-manylinux2010_x86_64.whl (1.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons==0.9.1) (2.7.1)\n",
            "Installing collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.9.1\n",
            "Collecting optuna\n",
            "  Downloading optuna-2.10.0-py3-none-any.whl (308 kB)\n",
            "\u001b[K     |████████████████████████████████| 308 kB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (21.3)\n",
            "Collecting cliff\n",
            "  Downloading cliff-3.10.1-py3-none-any.whl (81 kB)\n",
            "\u001b[K     |████████████████████████████████| 81 kB 8.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from optuna) (1.21.5)\n",
            "Collecting cmaes>=0.8.2\n",
            "  Downloading cmaes-0.8.2-py3-none-any.whl (15 kB)\n",
            "Collecting alembic\n",
            "  Downloading alembic-1.7.7-py3-none-any.whl (210 kB)\n",
            "\u001b[K     |████████████████████████████████| 210 kB 42.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from optuna) (4.63.0)\n",
            "Collecting colorlog\n",
            "  Downloading colorlog-6.6.0-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: scipy!=1.4.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (1.4.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from optuna) (3.13)\n",
            "Requirement already satisfied: sqlalchemy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (1.4.32)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->optuna) (3.0.7)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna) (4.11.3)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna) (1.1.2)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from alembic->optuna) (5.4.0)\n",
            "Collecting Mako\n",
            "  Downloading Mako-1.2.0-py3-none-any.whl (78 kB)\n",
            "\u001b[K     |████████████████████████████████| 78 kB 6.7 MB/s \n",
            "\u001b[?25hCollecting cmd2>=1.0.0\n",
            "  Downloading cmd2-2.4.0-py3-none-any.whl (150 kB)\n",
            "\u001b[K     |████████████████████████████████| 150 kB 49.2 MB/s \n",
            "\u001b[?25hCollecting stevedore>=2.0.1\n",
            "  Downloading stevedore-3.5.0-py3-none-any.whl (49 kB)\n",
            "\u001b[K     |████████████████████████████████| 49 kB 5.4 MB/s \n",
            "\u001b[?25hCollecting pbr!=2.1.0,>=2.0.0\n",
            "  Downloading pbr-5.8.1-py2.py3-none-any.whl (113 kB)\n",
            "\u001b[K     |████████████████████████████████| 113 kB 52.2 MB/s \n",
            "\u001b[?25hCollecting autopage>=0.4.0\n",
            "  Downloading autopage-0.5.0-py3-none-any.whl (29 kB)\n",
            "Requirement already satisfied: PrettyTable>=0.7.2 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna) (3.2.0)\n",
            "Collecting pyperclip>=1.6\n",
            "  Downloading pyperclip-1.8.2.tar.gz (20 kB)\n",
            "Requirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (0.2.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (3.10.0.2)\n",
            "Requirement already satisfied: attrs>=16.3.0 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (21.4.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->sqlalchemy>=1.1.0->optuna) (3.7.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from Mako->alembic->optuna) (2.0.1)\n",
            "Building wheels for collected packages: pyperclip\n",
            "  Building wheel for pyperclip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyperclip: filename=pyperclip-1.8.2-py3-none-any.whl size=11137 sha256=59870a8858e6a13716bbbff9efdc68797ddab3d89265cb76ff292ab3e5a3dd37\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/18/84/8f69f8b08169c7bae2dde6bd7daf0c19fca8c8e500ee620a28\n",
            "Successfully built pyperclip\n",
            "Installing collected packages: pyperclip, pbr, stevedore, Mako, cmd2, autopage, colorlog, cmaes, cliff, alembic, optuna\n",
            "Successfully installed Mako-1.2.0 alembic-1.7.7 autopage-0.5.0 cliff-3.10.1 cmaes-0.8.2 cmd2-2.4.0 colorlog-6.6.0 optuna-2.10.0 pbr-5.8.1 pyperclip-1.8.2 stevedore-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall tensorflow\n",
        "!pip install tensorflow==2.2.0\n",
        "!pip install tensorflow-addons==0.9.1\n",
        "!pip install optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yD8N4Qx0UWYb"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf ;tf.compat.v1.disable_eager_execution()\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow_hub as hub\n",
        "import optuna\n",
        "import gc\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import callbacks"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ItJ33yb62HeZ",
        "outputId": "8ae617e6-c8c2-485c-f4ec-1275064cc7bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRlYU535UchR"
      },
      "source": [
        "# Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-_VCDM85UfE9"
      },
      "outputs": [],
      "source": [
        "data_online_sess1 = np.load(f\"/content/drive/MyDrive/Brainwave_hack/data/phob_data/100Hz_data_online_sess1.npy\")\n",
        "data_online_sess2 = np.load(f\"/content/drive/MyDrive/Brainwave_hack/data/phob_data/100Hz_data_online_sess2.npy\")\n",
        "data_offline_sess1 = np.load(f\"/content/drive/MyDrive/Brainwave_hack/data/phob_data/100Hz_data_sess1.npy\")\n",
        "data_offline_sess2 = np.load(f\"/content/drive/MyDrive/Brainwave_hack/data/phob_data/100Hz_data_sess2.npy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mDuQZoiDUfyZ"
      },
      "outputs": [],
      "source": [
        "label_online_sess1 = np.load(f\"/content/drive/MyDrive/Brainwave_hack/data/phob_data/100Hz_label_online_sess1.npy\")\n",
        "label_online_sess2 = np.load(f\"/content/drive/MyDrive/Brainwave_hack/data/phob_data/100Hz_label_online_sess2.npy\")\n",
        "label_offline_sess1 = np.load(f\"/content/drive/MyDrive/Brainwave_hack/data/phob_data/100Hz_label_sess1.npy\")\n",
        "label_offline_sess2 = np.load(f\"/content/drive/MyDrive/Brainwave_hack/data/phob_data/100Hz_label_sess2.npy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hwunXhjGYLOm"
      },
      "outputs": [],
      "source": [
        "# concat data\n",
        "session_x_merge =  np.concatenate((data_online_sess1, data_online_sess2 , data_offline_sess1 , data_offline_sess2) , axis = 0)\n",
        "session_y_merge = np.concatenate((label_online_sess1, label_online_sess2 , label_offline_sess1 , label_offline_sess2) , axis = 0)\n",
        "del(data_online_sess1)\n",
        "del(data_online_sess2)\n",
        "del(data_offline_sess1) \n",
        "del(data_offline_sess2) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RTtEPa4pYTqc"
      },
      "outputs": [],
      "source": [
        "session_x_merge = np.transpose(session_x_merge , [0,2,1])\n",
        "session_x_merge = np.expand_dims(session_x_merge, axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iZOK-5y0YXV3",
        "outputId": "7d8fd5ac-a385-4ff2-e244-27837b2d7ff4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(62400, 1, 62, 400)\n"
          ]
        }
      ],
      "source": [
        "print(session_x_merge.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYOP5lOtYZEf",
        "outputId": "ab4edab2-5907-47be-ec89-28240e37e5d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of posters for training:  49920\n",
            "Number of posters for validation:  12480\n"
          ]
        }
      ],
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(session_x_merge, session_y_merge, test_size=0.2, random_state=44)\n",
        "print(\"Number of posters for training: \", len(X_train))\n",
        "print(\"Number of posters for validation: \", len(X_val))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pHYN-BDkYZfi"
      },
      "outputs": [],
      "source": [
        "del(session_x_merge)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqv8xJa7uQ00"
      },
      "source": [
        "# Altenative Bandpass data\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "session_x_merge =  np.expand_dims(np.concatenate((np.load(f\"/content/drive/MyDrive/Brain Powerrrrrrrr/offline_data_no_bandpass.npy\"), np.load(f\"/content/drive/MyDrive/Brain Powerrrrrrrr/online_data_no_bandpass.npy\")) , axis = 0),axis = 1)\n",
        "session_y_merge = np.concatenate((np.load(f\"/content/drive/MyDrive/Brain Powerrrrrrrr/offline_label_no_bandpass.npy\"), np.load(f\"/content/drive/MyDrive/Brain Powerrrrrrrr/online_label_no_bandpass.npy\")) , axis = 0)"
      ],
      "metadata": {
        "id": "gF88YUHiAVnR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SAIgwnJpupad",
        "outputId": "1ca208a8-871c-4790-ad06-e92b02371947"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(41600, 1, 62, 400)\n",
            "(41600,)\n"
          ]
        }
      ],
      "source": [
        "print(session_x_merge.shape)\n",
        "print(session_y_merge.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# #Select channel that have C\n",
        "# session_x_merge=session_x_merge[:,:,[7, 8, 9, 10, 12, 13, 14, 17, 18, 19, 20, 32, 33, 34, 35, 36, 37, 38, 39, 40],:]"
      ],
      "metadata": {
        "id": "5-DOLQO6s8dl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYQEAqdNvhYP",
        "outputId": "45a5972b-4931-4562-ff3a-0bf521aba803"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of posters for training:  33280\n",
            "Number of posters for validation:  8320\n"
          ]
        }
      ],
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(session_x_merge, session_y_merge, test_size=0.2, random_state=44)\n",
        "print(\"Number of posters for training: \", len(X_train))\n",
        "print(\"Number of posters for validation: \", len(X_val))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del(session_x_merge)\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWRSt5hq4chB",
        "outputId": "12151ced-f234-41c0-9442-6a99fa3c69c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "111"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJm9QWEhZ60f"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv2D, Dense, Flatten, Dropout, BatchNormalization, Activation, MaxPooling2D\n",
        "from tensorflow.keras.constraints import max_norm\n",
        "from tensorflow.keras.callbacks import CSVLogger, ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "from sklearn.metrics import classification_report, f1_score"
      ],
      "metadata": {
        "id": "bxBSMkhn8kqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7eeDmw7mZ89y"
      },
      "outputs": [],
      "source": [
        "class DeepConvNet:\n",
        "    def __init__(self,\n",
        "                input_shape=(1,20,400),\n",
        "                num_class=2,\n",
        "                loss='sparse_categorical_crossentropy',\n",
        "                epochs=200,\n",
        "                batch_size=100,\n",
        "                optimizer = Adam(beta_1=0.9, beta_2=0.999, epsilon=1e-08),\n",
        "                lr=0.01,\n",
        "                min_lr=0.01,\n",
        "                factor=0.25,\n",
        "                patience=10,\n",
        "                es_patience=20,\n",
        "                verbose=1,\n",
        "                log_path='logs',\n",
        "                model_name='DeepConvNet',\n",
        "                **kwargs):\n",
        "        self.input_shape = input_shape\n",
        "        self.num_class = num_class\n",
        "        self.loss = loss\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.optimizer = optimizer\n",
        "        self.optimizer.lr = lr\n",
        "        self.lr = lr\n",
        "        self.min_lr = min_lr\n",
        "        self.factor = factor\n",
        "        self.patience = patience\n",
        "        self.es_patience = es_patience\n",
        "        self.verbose = verbose\n",
        "        self.log_path = log_path\n",
        "        self.model_name = model_name\n",
        "        self.weights_dir = log_path+'/'+model_name+'_out_weights.h5'\n",
        "        self.csv_dir = log_path+'/'+model_name+'_out_log.log'\n",
        "        self.time_log = log_path+'/'+model_name+'_time_log.csv'\n",
        "\n",
        "        # use **kwargs to set the new value of below args.\n",
        "        self.kernLength = 125\n",
        "        self.norm_rate = 0.25\n",
        "        self.dropout_rate = 0.5\n",
        "        self.data_format = 'channels_first'\n",
        "        self.shuffle = False\n",
        "        self.metrics = 'accuracy'\n",
        "        self.monitor = 'val_loss'\n",
        "        self.mode = 'min'\n",
        "        self.save_best_only = True\n",
        "        self.save_weight_only = True\n",
        "        self.seed = 1234\n",
        "        self.class_balancing = False\n",
        "        self.class_weight = None\n",
        "\n",
        "        for k in kwargs.keys():\n",
        "            self.__setattr__(k, kwargs[k])\n",
        "            \n",
        "        if self.data_format == 'channels_first':\n",
        "            self.Chans = self.input_shape[1]\n",
        "            self.Samples = self.input_shape[2]\n",
        "        else:\n",
        "            self.Chans = self.input_shape[0]\n",
        "            self.Samples = self.input_shape[1]\n",
        "\n",
        "        np.random.seed(self.seed)\n",
        "        tf.random.set_seed(self.seed)\n",
        "        K.set_image_data_format(self.data_format)\n",
        "        if not os.path.exists(self.log_path):\n",
        "            os.makedirs(self.log_path)\n",
        "\n",
        "    def build(self):\n",
        "        \"\"\" Keras implementation of the Deep Convolutional Network as described in\n",
        "        Schirrmeister et. al. (2017), Human Brain Mapping.\n",
        "        This implementation assumes the input is a 2-second EEG signal sampled at\n",
        "        128Hz, as opposed to signals sampled at 250Hz as described in the original\n",
        "        paper. We also perform temporal convolutions of length (1, 5) as opposed\n",
        "        to (1, 10) due to this sampling rate difference.\n",
        "        Note that we use the max_norm constraint on all convolutional layers, as\n",
        "        well as the classification layer. We also change the defaults for the\n",
        "        BatchNormalization layer. We used this based on a personal communication\n",
        "        with the original authors.\n",
        "                          ours        original paper\n",
        "        pool_size        1, 2        1, 3\n",
        "        strides          1, 2        1, 3\n",
        "        conv filters     1, 5        1, 10\n",
        "        Note that this implementation has not been verified by the original\n",
        "        authors.\n",
        "        \"\"\"\n",
        "\n",
        "        # start the model\n",
        "        input_main   = Input(self.input_shape)\n",
        "        block1       = Conv2D(25, (1, 5),\n",
        "                              input_shape=(self.input_shape),\n",
        "                              kernel_constraint=max_norm(2., axis=(0, 1, 2)))(input_main)\n",
        "        block1       = Conv2D(25, (self.Chans, 1),\n",
        "                              kernel_constraint=max_norm(2., axis=(0, 1, 2)))(block1)\n",
        "        block1       = BatchNormalization(axis=1, epsilon=1e-05, momentum=0.1)(block1)\n",
        "        block1       = Activation('elu')(block1)\n",
        "        block1       = MaxPooling2D(pool_size=(1, 2), strides=(1, 2))(block1)\n",
        "        block1       = Dropout(self.dropout_rate)(block1)\n",
        "\n",
        "        block2       = Conv2D(50, (1, 5),\n",
        "                              kernel_constraint=max_norm(2., axis=(0, 1, 2)))(block1)\n",
        "        block2       = BatchNormalization(axis=1, epsilon=1e-05, momentum=0.1)(block2)\n",
        "        block2       = Activation('elu')(block2)\n",
        "        block2       = MaxPooling2D(pool_size=(1, 2), strides=(1, 2))(block2)\n",
        "        block2       = Dropout(self.dropout_rate)(block2)\n",
        "\n",
        "        block3       = Conv2D(100, (1, 5),\n",
        "                              kernel_constraint=max_norm(2., axis=(0, 1, 2)))(block2)\n",
        "        block3       = BatchNormalization(axis=1, epsilon=1e-05, momentum=0.1)(block3)\n",
        "        block3       = Activation('elu')(block3)\n",
        "        block3       = MaxPooling2D(pool_size=(1, 2), strides=(1, 2))(block3)\n",
        "        block3       = Dropout(self.dropout_rate)(block3)\n",
        "\n",
        "        block4       = Conv2D(200, (1, 5),\n",
        "                              kernel_constraint=max_norm(2., axis=(0, 1, 2)))(block3)\n",
        "        block4       = BatchNormalization(axis=1, epsilon=1e-05, momentum=0.1)(block4)\n",
        "        block4       = Activation('elu')(block4)\n",
        "        block4       = MaxPooling2D(pool_size=(1, 2), strides=(1, 2))(block4)\n",
        "        block4       = Dropout(self.dropout_rate)(block4)\n",
        "\n",
        "        flatten      = Flatten()(block4)\n",
        "\n",
        "        dense        = Dense(self.num_class, kernel_constraint = max_norm(0.5))(flatten)\n",
        "        softmax      = Activation('softmax')(dense)\n",
        "\n",
        "        return Model(inputs=input_main, outputs=softmax)\n",
        "\n",
        "    def fit(self, X_train, y_train, X_val, y_val):\n",
        "\n",
        "        if X_train.ndim != 4:\n",
        "            raise Exception('ValueError: `X_train` is incompatible: expected ndim=4, found ndim='+str(X_train.ndim))\n",
        "        elif X_val.ndim != 4:\n",
        "            raise Exception('ValueError: `X_val` is incompatible: expected ndim=4, found ndim='+str(X_val.ndim))\n",
        "\n",
        "        self.input_shape = X_train.shape[1:]\n",
        "        if self.data_format == 'channels_first':\n",
        "            self.Chans = self.input_shape[1]\n",
        "            self.Samples = self.input_shape[2]\n",
        "        else:\n",
        "            self.Chans = self.input_shape[0]\n",
        "            self.Samples = self.input_shape[1]\n",
        "        \n",
        "        csv_logger = CSVLogger(self.csv_dir)\n",
        "        checkpointer = ModelCheckpoint(monitor=self.monitor, filepath=self.weights_dir, verbose=self.verbose, \n",
        "                                       save_best_only=self.save_best_only, save_weight_only=self.save_weight_only)\n",
        "        reduce_lr = ReduceLROnPlateau(monitor=self.monitor, patience=self.patience, factor=self.factor, mode=self.mode, \n",
        "                                      verbose=self.verbose, min_lr=self.min_lr)\n",
        "        es = EarlyStopping(monitor=self.monitor, mode=self.mode, verbose=self.verbose, patience=self.es_patience)\n",
        "\n",
        "        model = self.build()\n",
        "        model.compile(optimizer=self.optimizer, loss=self.loss, metrics=[self.metrics])\n",
        "        model.summary()\n",
        "        \n",
        "            \n",
        "        model.fit(X_train, y_train,\n",
        "                  batch_size=self.batch_size, shuffle=self.shuffle,\n",
        "                  epochs=self.epochs, validation_data=(X_val, y_val), class_weight=self.class_weight,\n",
        "                  callbacks=[checkpointer,csv_logger,reduce_lr,es])\n",
        "    def fit_continue(self, X_train, y_train, X_val, y_val):\n",
        "\n",
        "        if X_train.ndim != 4:\n",
        "            raise Exception('ValueError: `X_train` is incompatible: expected ndim=4, found ndim='+str(X_train.ndim))\n",
        "        elif X_val.ndim != 4:\n",
        "            raise Exception('ValueError: `X_val` is incompatible: expected ndim=4, found ndim='+str(X_val.ndim))\n",
        "\n",
        "        self.input_shape = X_train.shape[1:]\n",
        "        if self.data_format == 'channels_first':\n",
        "            self.Chans = self.input_shape[1]\n",
        "            self.Samples = self.input_shape[2]\n",
        "        else:\n",
        "            self.Chans = self.input_shape[0]\n",
        "            self.Samples = self.input_shape[1]\n",
        "        \n",
        "        csv_logger = CSVLogger(self.csv_dir)\n",
        "        checkpointer = ModelCheckpoint(monitor=self.monitor, filepath=self.weights_dir, verbose=self.verbose, \n",
        "                                       save_best_only=self.save_best_only, save_weight_only=self.save_weight_only)\n",
        "        reduce_lr = ReduceLROnPlateau(monitor=self.monitor, patience=self.patience, factor=self.factor, mode=self.mode, \n",
        "                                      verbose=self.verbose, min_lr=self.min_lr)\n",
        "        es = EarlyStopping(monitor=self.monitor, mode=self.mode, verbose=self.verbose, patience=self.es_patience)\n",
        "\n",
        "        model = self.build()\n",
        "        model.load_weights(self.weights_dir)\n",
        "        model.compile(optimizer=self.optimizer, loss=self.loss, metrics=[self.metrics])\n",
        "        model.summary()\n",
        "        \n",
        "            \n",
        "        model.fit(X_train, y_train,\n",
        "                  batch_size=self.batch_size, shuffle=self.shuffle,\n",
        "                  epochs=self.epochs, validation_data=(X_val, y_val), class_weight=self.class_weight,\n",
        "                  callbacks=[checkpointer,csv_logger,reduce_lr,es])\n",
        "\n",
        "    def predict(self, X_test):\n",
        "\n",
        "        if X_test.ndim != 4:\n",
        "            raise Exception('ValueError: `X_test` is incompatible: expected ndim=4, found ndim='+str(X_test.ndim))\n",
        "\n",
        "        model = self.build()\n",
        "        model.load_weights(self.weights_dir)\n",
        "        model.compile(optimizer=self.optimizer, loss=self.loss, metrics=[self.metrics])\n",
        "        model.summary()\n",
        "\n",
        "        y_pred = model.predict(X_test)\n",
        "        y_pred_argm = np.argmax(y_pred, axis=1)\n",
        "        return y_pred_argm "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def objective(trial):\n",
        "  dropout_rate=trial.suggest_float('dropout_rate', 0.1, 0.5, log=True)\n",
        "  batch_size=trial.suggest_int('batch_size', 128, 1024, log=True)\n",
        "\n",
        "  model=DeepConvNet(input_shape=(1,62,400), num_class=4, dropout_rate=dropout_rate, shuffle=True).build()\n",
        "\n",
        "  checkpointer = ModelCheckpoint(monitor='val_accuracy', filepath='/content/drive/MyDrive/Brain Powerrrrrrrr/Save_weight/DeepConvNetOptuna.h5', verbose=1, \n",
        "                                       save_best_only=True, save_weight_only=True)\n",
        "  reduce_lr = ReduceLROnPlateau(monitor='val_accuracy', patience=10, factor=0.25, mode='max', \n",
        "                                      verbose=1, min_lr=1e-5)\n",
        "  es = EarlyStopping(monitor='val_accuracy', mode='max', verbose=1, patience=20)\n",
        "\n",
        "  model.compile(optimizer=Adam(beta_1=0.9, beta_2=0.999, epsilon=1e-08,learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "  # model.fit_generator(generator=training_generator,\n",
        "  #                   validation_data=validation_generator,\n",
        "  #                   workers=6,epochs=400,callbacks=[checkpointer,reduce_lr,es])\n",
        "  model.fit(X_train,y_train,validation_data=(X_val, y_val),shuffle=True,batch_size=batch_size,epochs=400,callbacks=[checkpointer,reduce_lr,es])\n",
        "\n",
        "  score = model.evaluate(X_val, y_val, verbose=0)\n",
        "\n",
        "  return score[1]\n",
        "\n",
        "study = optuna.create_study(direction='maximize')\n",
        "study.optimize(objective, n_trials=100)"
      ],
      "metadata": {
        "id": "z99k8Y6-3lI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mGyZhXN8fSfL"
      },
      "outputs": [],
      "source": [
        "model = DeepConvNet(input_shape=(1,62,400), num_class=4, dropout_rate=0.25, shuffle=True , batch_size= 1024 , lr=0.001,epochs=400,min_lr=1e-5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eMTHGnoBfc-y",
        "outputId": "b65b25d8-97e9-475a-9a45-6754993a7e92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/resource_variable_ops.py:1666: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 1, 62, 400)]      0         \n",
            "_________________________________________________________________\n",
            "conv2d (Conv2D)              (None, 25, 62, 396)       150       \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 25, 1, 396)        38775     \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 25, 1, 396)        100       \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 25, 1, 396)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 25, 1, 198)        0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 25, 1, 198)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 50, 1, 194)        6300      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 50, 1, 194)        200       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 50, 1, 194)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 50, 1, 97)         0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 50, 1, 97)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 100, 1, 93)        25100     \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 100, 1, 93)        400       \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 100, 1, 93)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 100, 1, 46)        0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 100, 1, 46)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 200, 1, 42)        100200    \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 200, 1, 42)        800       \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 200, 1, 42)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 200, 1, 21)        0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 200, 1, 21)        0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 4200)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 4)                 16804     \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 4)                 0         \n",
            "=================================================================\n",
            "Total params: 188,829\n",
            "Trainable params: 188,079\n",
            "Non-trainable params: 750\n",
            "_________________________________________________________________\n",
            "Train on 33280 samples, validate on 8320 samples\n",
            "Epoch 1/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 1.3034 - accuracy: 0.4258\n",
            "Epoch 00001: val_loss improved from inf to 1.07257, saving model to logs/DeepConvNet_out_weights.h5\n",
            "33280/33280 [==============================] - 12s 356us/sample - loss: 1.3034 - accuracy: 0.4258 - val_loss: 1.0726 - val_accuracy: 0.5468 - lr: 0.0010\n",
            "Epoch 2/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 1.0237 - accuracy: 0.5508\n",
            "Epoch 00002: val_loss improved from 1.07257 to 0.91952, saving model to logs/DeepConvNet_out_weights.h5\n",
            "33280/33280 [==============================] - 10s 313us/sample - loss: 1.0237 - accuracy: 0.5508 - val_loss: 0.9195 - val_accuracy: 0.6001 - lr: 0.0010\n",
            "Epoch 3/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.9071 - accuracy: 0.5939\n",
            "Epoch 00003: val_loss improved from 0.91952 to 0.80594, saving model to logs/DeepConvNet_out_weights.h5\n",
            "33280/33280 [==============================] - 10s 313us/sample - loss: 0.9071 - accuracy: 0.5939 - val_loss: 0.8059 - val_accuracy: 0.6412 - lr: 0.0010\n",
            "Epoch 4/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.8347 - accuracy: 0.6231\n",
            "Epoch 00004: val_loss improved from 0.80594 to 0.75034, saving model to logs/DeepConvNet_out_weights.h5\n",
            "33280/33280 [==============================] - 10s 312us/sample - loss: 0.8347 - accuracy: 0.6231 - val_loss: 0.7503 - val_accuracy: 0.6578 - lr: 0.0010\n",
            "Epoch 5/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.7917 - accuracy: 0.6340\n",
            "Epoch 00005: val_loss improved from 0.75034 to 0.71864, saving model to logs/DeepConvNet_out_weights.h5\n",
            "33280/33280 [==============================] - 10s 312us/sample - loss: 0.7917 - accuracy: 0.6340 - val_loss: 0.7186 - val_accuracy: 0.6680 - lr: 0.0010\n",
            "Epoch 6/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.7645 - accuracy: 0.6474\n",
            "Epoch 00006: val_loss improved from 0.71864 to 0.68955, saving model to logs/DeepConvNet_out_weights.h5\n",
            "33280/33280 [==============================] - 10s 314us/sample - loss: 0.7645 - accuracy: 0.6474 - val_loss: 0.6896 - val_accuracy: 0.6794 - lr: 0.0010\n",
            "Epoch 7/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.7279 - accuracy: 0.6592\n",
            "Epoch 00007: val_loss improved from 0.68955 to 0.66515, saving model to logs/DeepConvNet_out_weights.h5\n",
            "33280/33280 [==============================] - 10s 312us/sample - loss: 0.7279 - accuracy: 0.6592 - val_loss: 0.6652 - val_accuracy: 0.6904 - lr: 0.0010\n",
            "Epoch 8/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.7060 - accuracy: 0.6724\n",
            "Epoch 00008: val_loss improved from 0.66515 to 0.64010, saving model to logs/DeepConvNet_out_weights.h5\n",
            "33280/33280 [==============================] - 10s 311us/sample - loss: 0.7060 - accuracy: 0.6724 - val_loss: 0.6401 - val_accuracy: 0.7019 - lr: 0.0010\n",
            "Epoch 9/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.6737 - accuracy: 0.6839\n",
            "Epoch 00009: val_loss improved from 0.64010 to 0.62042, saving model to logs/DeepConvNet_out_weights.h5\n",
            "33280/33280 [==============================] - 10s 312us/sample - loss: 0.6737 - accuracy: 0.6839 - val_loss: 0.6204 - val_accuracy: 0.7043 - lr: 0.0010\n",
            "Epoch 10/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.6513 - accuracy: 0.6914\n",
            "Epoch 00010: val_loss improved from 0.62042 to 0.58339, saving model to logs/DeepConvNet_out_weights.h5\n",
            "33280/33280 [==============================] - 10s 312us/sample - loss: 0.6513 - accuracy: 0.6914 - val_loss: 0.5834 - val_accuracy: 0.7272 - lr: 0.0010\n",
            "Epoch 11/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.6174 - accuracy: 0.7106\n",
            "Epoch 00011: val_loss improved from 0.58339 to 0.54012, saving model to logs/DeepConvNet_out_weights.h5\n",
            "33280/33280 [==============================] - 10s 311us/sample - loss: 0.6174 - accuracy: 0.7106 - val_loss: 0.5401 - val_accuracy: 0.7572 - lr: 0.0010\n",
            "Epoch 12/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.5724 - accuracy: 0.7376\n",
            "Epoch 00012: val_loss improved from 0.54012 to 0.50793, saving model to logs/DeepConvNet_out_weights.h5\n",
            "33280/33280 [==============================] - 10s 311us/sample - loss: 0.5724 - accuracy: 0.7376 - val_loss: 0.5079 - val_accuracy: 0.7728 - lr: 0.0010\n",
            "Epoch 13/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.5437 - accuracy: 0.7563\n",
            "Epoch 00013: val_loss improved from 0.50793 to 0.48635, saving model to logs/DeepConvNet_out_weights.h5\n",
            "33280/33280 [==============================] - 10s 315us/sample - loss: 0.5437 - accuracy: 0.7563 - val_loss: 0.4864 - val_accuracy: 0.7857 - lr: 0.0010\n",
            "Epoch 14/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.5124 - accuracy: 0.7715\n",
            "Epoch 00014: val_loss improved from 0.48635 to 0.46271, saving model to logs/DeepConvNet_out_weights.h5\n",
            "33280/33280 [==============================] - 10s 313us/sample - loss: 0.5124 - accuracy: 0.7715 - val_loss: 0.4627 - val_accuracy: 0.7969 - lr: 0.0010\n",
            "Epoch 15/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.4970 - accuracy: 0.7820\n",
            "Epoch 00015: val_loss improved from 0.46271 to 0.45060, saving model to logs/DeepConvNet_out_weights.h5\n",
            "33280/33280 [==============================] - 10s 311us/sample - loss: 0.4970 - accuracy: 0.7820 - val_loss: 0.4506 - val_accuracy: 0.8069 - lr: 0.0010\n",
            "Epoch 16/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.4805 - accuracy: 0.7892\n",
            "Epoch 00016: val_loss improved from 0.45060 to 0.44942, saving model to logs/DeepConvNet_out_weights.h5\n",
            "33280/33280 [==============================] - 10s 309us/sample - loss: 0.4805 - accuracy: 0.7892 - val_loss: 0.4494 - val_accuracy: 0.8055 - lr: 0.0010\n",
            "Epoch 17/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.4624 - accuracy: 0.7998\n",
            "Epoch 00017: val_loss improved from 0.44942 to 0.43160, saving model to logs/DeepConvNet_out_weights.h5\n",
            "33280/33280 [==============================] - 10s 311us/sample - loss: 0.4624 - accuracy: 0.7998 - val_loss: 0.4316 - val_accuracy: 0.8142 - lr: 0.0010\n",
            "Epoch 18/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.4483 - accuracy: 0.8052\n",
            "Epoch 00018: val_loss improved from 0.43160 to 0.41646, saving model to logs/DeepConvNet_out_weights.h5\n",
            "33280/33280 [==============================] - 10s 310us/sample - loss: 0.4483 - accuracy: 0.8052 - val_loss: 0.4165 - val_accuracy: 0.8237 - lr: 0.0010\n",
            "Epoch 19/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.4246 - accuracy: 0.8179\n",
            "Epoch 00019: val_loss improved from 0.41646 to 0.39460, saving model to logs/DeepConvNet_out_weights.h5\n",
            "33280/33280 [==============================] - 10s 309us/sample - loss: 0.4246 - accuracy: 0.8179 - val_loss: 0.3946 - val_accuracy: 0.8331 - lr: 0.0010\n",
            "Epoch 20/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.4163 - accuracy: 0.8227\n",
            "Epoch 00020: val_loss improved from 0.39460 to 0.38884, saving model to logs/DeepConvNet_out_weights.h5\n",
            "33280/33280 [==============================] - 10s 311us/sample - loss: 0.4163 - accuracy: 0.8227 - val_loss: 0.3888 - val_accuracy: 0.8357 - lr: 0.0010\n",
            "Epoch 21/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.4044 - accuracy: 0.8292\n",
            "Epoch 00021: val_loss improved from 0.38884 to 0.38265, saving model to logs/DeepConvNet_out_weights.h5\n",
            "33280/33280 [==============================] - 10s 312us/sample - loss: 0.4044 - accuracy: 0.8292 - val_loss: 0.3827 - val_accuracy: 0.8377 - lr: 0.0010\n",
            "Epoch 22/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.3971 - accuracy: 0.8323\n",
            "Epoch 00022: val_loss improved from 0.38265 to 0.37059, saving model to logs/DeepConvNet_out_weights.h5\n",
            "33280/33280 [==============================] - 10s 311us/sample - loss: 0.3971 - accuracy: 0.8323 - val_loss: 0.3706 - val_accuracy: 0.8452 - lr: 0.0010\n",
            "Epoch 23/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.3848 - accuracy: 0.8383\n",
            "Epoch 00023: val_loss improved from 0.37059 to 0.36387, saving model to logs/DeepConvNet_out_weights.h5\n",
            "33280/33280 [==============================] - 10s 310us/sample - loss: 0.3848 - accuracy: 0.8383 - val_loss: 0.3639 - val_accuracy: 0.8457 - lr: 0.0010\n",
            "Epoch 24/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.3798 - accuracy: 0.8407\n",
            "Epoch 00024: val_loss improved from 0.36387 to 0.36383, saving model to logs/DeepConvNet_out_weights.h5\n",
            "33280/33280 [==============================] - 10s 309us/sample - loss: 0.3798 - accuracy: 0.8407 - val_loss: 0.3638 - val_accuracy: 0.8517 - lr: 0.0010\n",
            "Epoch 25/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.3671 - accuracy: 0.8471\n",
            "Epoch 00025: val_loss improved from 0.36383 to 0.34793, saving model to logs/DeepConvNet_out_weights.h5\n",
            "33280/33280 [==============================] - 10s 310us/sample - loss: 0.3671 - accuracy: 0.8471 - val_loss: 0.3479 - val_accuracy: 0.8588 - lr: 0.0010\n",
            "Epoch 26/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.3561 - accuracy: 0.8519\n",
            "Epoch 00026: val_loss improved from 0.34793 to 0.34437, saving model to logs/DeepConvNet_out_weights.h5\n",
            "33280/33280 [==============================] - 10s 310us/sample - loss: 0.3561 - accuracy: 0.8519 - val_loss: 0.3444 - val_accuracy: 0.8583 - lr: 0.0010\n",
            "Epoch 27/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.3547 - accuracy: 0.8536\n",
            "Epoch 00027: val_loss improved from 0.34437 to 0.34414, saving model to logs/DeepConvNet_out_weights.h5\n",
            "33280/33280 [==============================] - 10s 310us/sample - loss: 0.3547 - accuracy: 0.8536 - val_loss: 0.3441 - val_accuracy: 0.8559 - lr: 0.0010\n",
            "Epoch 28/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.3523 - accuracy: 0.8533\n",
            "Epoch 00028: val_loss improved from 0.34414 to 0.33456, saving model to logs/DeepConvNet_out_weights.h5\n",
            "33280/33280 [==============================] - 10s 311us/sample - loss: 0.3523 - accuracy: 0.8533 - val_loss: 0.3346 - val_accuracy: 0.8643 - lr: 0.0010\n",
            "Epoch 29/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.3449 - accuracy: 0.8580\n",
            "Epoch 00029: val_loss did not improve from 0.33456\n",
            "33280/33280 [==============================] - 10s 308us/sample - loss: 0.3449 - accuracy: 0.8580 - val_loss: 0.3434 - val_accuracy: 0.8581 - lr: 0.0010\n",
            "Epoch 30/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.3413 - accuracy: 0.8603\n",
            "Epoch 00030: val_loss did not improve from 0.33456\n",
            "33280/33280 [==============================] - 10s 307us/sample - loss: 0.3413 - accuracy: 0.8603 - val_loss: 0.3372 - val_accuracy: 0.8617 - lr: 0.0010\n",
            "Epoch 31/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.3340 - accuracy: 0.8626\n",
            "Epoch 00031: val_loss did not improve from 0.33456\n",
            "33280/33280 [==============================] - 10s 308us/sample - loss: 0.3340 - accuracy: 0.8626 - val_loss: 0.3348 - val_accuracy: 0.8607 - lr: 0.0010\n",
            "Epoch 32/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.3301 - accuracy: 0.8623\n",
            "Epoch 00032: val_loss improved from 0.33456 to 0.32830, saving model to logs/DeepConvNet_out_weights.h5\n",
            "33280/33280 [==============================] - 10s 310us/sample - loss: 0.3301 - accuracy: 0.8623 - val_loss: 0.3283 - val_accuracy: 0.8660 - lr: 0.0010\n",
            "Epoch 33/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.3265 - accuracy: 0.8657\n",
            "Epoch 00033: val_loss improved from 0.32830 to 0.32708, saving model to logs/DeepConvNet_out_weights.h5\n",
            "33280/33280 [==============================] - 10s 311us/sample - loss: 0.3265 - accuracy: 0.8657 - val_loss: 0.3271 - val_accuracy: 0.8666 - lr: 0.0010\n",
            "Epoch 34/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.3157 - accuracy: 0.8698\n",
            "Epoch 00034: val_loss improved from 0.32708 to 0.32322, saving model to logs/DeepConvNet_out_weights.h5\n",
            "33280/33280 [==============================] - 10s 309us/sample - loss: 0.3157 - accuracy: 0.8698 - val_loss: 0.3232 - val_accuracy: 0.8691 - lr: 0.0010\n",
            "Epoch 35/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.3179 - accuracy: 0.8694\n",
            "Epoch 00035: val_loss improved from 0.32322 to 0.31891, saving model to logs/DeepConvNet_out_weights.h5\n",
            "33280/33280 [==============================] - 10s 310us/sample - loss: 0.3179 - accuracy: 0.8694 - val_loss: 0.3189 - val_accuracy: 0.8721 - lr: 0.0010\n",
            "Epoch 36/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.3131 - accuracy: 0.8720\n",
            "Epoch 00036: val_loss did not improve from 0.31891\n",
            "33280/33280 [==============================] - 10s 309us/sample - loss: 0.3131 - accuracy: 0.8720 - val_loss: 0.3330 - val_accuracy: 0.8663 - lr: 0.0010\n",
            "Epoch 37/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.3078 - accuracy: 0.8758\n",
            "Epoch 00037: val_loss improved from 0.31891 to 0.31380, saving model to logs/DeepConvNet_out_weights.h5\n",
            "33280/33280 [==============================] - 10s 311us/sample - loss: 0.3078 - accuracy: 0.8758 - val_loss: 0.3138 - val_accuracy: 0.8692 - lr: 0.0010\n",
            "Epoch 38/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.3061 - accuracy: 0.8767\n",
            "Epoch 00038: val_loss improved from 0.31380 to 0.31296, saving model to logs/DeepConvNet_out_weights.h5\n",
            "33280/33280 [==============================] - 10s 311us/sample - loss: 0.3061 - accuracy: 0.8767 - val_loss: 0.3130 - val_accuracy: 0.8748 - lr: 0.0010\n",
            "Epoch 39/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.3001 - accuracy: 0.8774\n",
            "Epoch 00039: val_loss did not improve from 0.31296\n",
            "33280/33280 [==============================] - 10s 306us/sample - loss: 0.3001 - accuracy: 0.8774 - val_loss: 0.3218 - val_accuracy: 0.8667 - lr: 0.0010\n",
            "Epoch 40/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.3015 - accuracy: 0.8767\n",
            "Epoch 00040: val_loss improved from 0.31296 to 0.30322, saving model to logs/DeepConvNet_out_weights.h5\n",
            "33280/33280 [==============================] - 10s 310us/sample - loss: 0.3015 - accuracy: 0.8767 - val_loss: 0.3032 - val_accuracy: 0.8763 - lr: 0.0010\n",
            "Epoch 41/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.3016 - accuracy: 0.8764\n",
            "Epoch 00041: val_loss did not improve from 0.30322\n",
            "33280/33280 [==============================] - 10s 307us/sample - loss: 0.3016 - accuracy: 0.8764 - val_loss: 0.3119 - val_accuracy: 0.8730 - lr: 0.0010\n",
            "Epoch 42/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.2990 - accuracy: 0.8787\n",
            "Epoch 00042: val_loss did not improve from 0.30322\n",
            "33280/33280 [==============================] - 10s 309us/sample - loss: 0.2990 - accuracy: 0.8787 - val_loss: 0.3109 - val_accuracy: 0.8778 - lr: 0.0010\n",
            "Epoch 43/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.2913 - accuracy: 0.8813\n",
            "Epoch 00043: val_loss did not improve from 0.30322\n",
            "33280/33280 [==============================] - 10s 309us/sample - loss: 0.2913 - accuracy: 0.8813 - val_loss: 0.3117 - val_accuracy: 0.8721 - lr: 0.0010\n",
            "Epoch 44/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.2941 - accuracy: 0.8798\n",
            "Epoch 00044: val_loss did not improve from 0.30322\n",
            "33280/33280 [==============================] - 10s 308us/sample - loss: 0.2941 - accuracy: 0.8798 - val_loss: 0.3125 - val_accuracy: 0.8738 - lr: 0.0010\n",
            "Epoch 45/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.2926 - accuracy: 0.8809\n",
            "Epoch 00045: val_loss did not improve from 0.30322\n",
            "33280/33280 [==============================] - 10s 308us/sample - loss: 0.2926 - accuracy: 0.8809 - val_loss: 0.3093 - val_accuracy: 0.8727 - lr: 0.0010\n",
            "Epoch 46/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.2794 - accuracy: 0.8878\n",
            "Epoch 00046: val_loss improved from 0.30322 to 0.30016, saving model to logs/DeepConvNet_out_weights.h5\n",
            "33280/33280 [==============================] - 10s 310us/sample - loss: 0.2794 - accuracy: 0.8878 - val_loss: 0.3002 - val_accuracy: 0.8770 - lr: 0.0010\n",
            "Epoch 47/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.2783 - accuracy: 0.8874\n",
            "Epoch 00047: val_loss did not improve from 0.30016\n",
            "33280/33280 [==============================] - 10s 309us/sample - loss: 0.2783 - accuracy: 0.8874 - val_loss: 0.3044 - val_accuracy: 0.8778 - lr: 0.0010\n",
            "Epoch 48/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.2773 - accuracy: 0.8878\n",
            "Epoch 00048: val_loss improved from 0.30016 to 0.29211, saving model to logs/DeepConvNet_out_weights.h5\n",
            "33280/33280 [==============================] - 10s 311us/sample - loss: 0.2773 - accuracy: 0.8878 - val_loss: 0.2921 - val_accuracy: 0.8820 - lr: 0.0010\n",
            "Epoch 49/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.2768 - accuracy: 0.8879\n",
            "Epoch 00049: val_loss improved from 0.29211 to 0.29208, saving model to logs/DeepConvNet_out_weights.h5\n",
            "33280/33280 [==============================] - 10s 311us/sample - loss: 0.2768 - accuracy: 0.8879 - val_loss: 0.2921 - val_accuracy: 0.8822 - lr: 0.0010\n",
            "Epoch 50/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.2761 - accuracy: 0.8878\n",
            "Epoch 00050: val_loss improved from 0.29208 to 0.29199, saving model to logs/DeepConvNet_out_weights.h5\n",
            "33280/33280 [==============================] - 10s 311us/sample - loss: 0.2761 - accuracy: 0.8878 - val_loss: 0.2920 - val_accuracy: 0.8845 - lr: 0.0010\n",
            "Epoch 51/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.2702 - accuracy: 0.8902\n",
            "Epoch 00051: val_loss did not improve from 0.29199\n",
            "33280/33280 [==============================] - 10s 309us/sample - loss: 0.2702 - accuracy: 0.8902 - val_loss: 0.2956 - val_accuracy: 0.8794 - lr: 0.0010\n",
            "Epoch 52/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.2724 - accuracy: 0.8899\n",
            "Epoch 00052: val_loss did not improve from 0.29199\n",
            "33280/33280 [==============================] - 10s 307us/sample - loss: 0.2724 - accuracy: 0.8899 - val_loss: 0.2954 - val_accuracy: 0.8808 - lr: 0.0010\n",
            "Epoch 53/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.2694 - accuracy: 0.8913\n",
            "Epoch 00053: val_loss improved from 0.29199 to 0.28976, saving model to logs/DeepConvNet_out_weights.h5\n",
            "33280/33280 [==============================] - 10s 310us/sample - loss: 0.2694 - accuracy: 0.8913 - val_loss: 0.2898 - val_accuracy: 0.8850 - lr: 0.0010\n",
            "Epoch 54/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.2726 - accuracy: 0.8898\n",
            "Epoch 00054: val_loss did not improve from 0.28976\n",
            "33280/33280 [==============================] - 10s 310us/sample - loss: 0.2726 - accuracy: 0.8898 - val_loss: 0.2953 - val_accuracy: 0.8787 - lr: 0.0010\n",
            "Epoch 55/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.2649 - accuracy: 0.8934\n",
            "Epoch 00055: val_loss did not improve from 0.28976\n",
            "33280/33280 [==============================] - 10s 309us/sample - loss: 0.2649 - accuracy: 0.8934 - val_loss: 0.2937 - val_accuracy: 0.8821 - lr: 0.0010\n",
            "Epoch 56/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.2643 - accuracy: 0.8913\n",
            "Epoch 00056: val_loss improved from 0.28976 to 0.28776, saving model to logs/DeepConvNet_out_weights.h5\n",
            "33280/33280 [==============================] - 10s 313us/sample - loss: 0.2643 - accuracy: 0.8913 - val_loss: 0.2878 - val_accuracy: 0.8846 - lr: 0.0010\n",
            "Epoch 57/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.2630 - accuracy: 0.8962\n",
            "Epoch 00057: val_loss improved from 0.28776 to 0.28506, saving model to logs/DeepConvNet_out_weights.h5\n",
            "33280/33280 [==============================] - 10s 313us/sample - loss: 0.2630 - accuracy: 0.8962 - val_loss: 0.2851 - val_accuracy: 0.8844 - lr: 0.0010\n",
            "Epoch 58/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.2576 - accuracy: 0.8983\n",
            "Epoch 00058: val_loss did not improve from 0.28506\n",
            "33280/33280 [==============================] - 10s 309us/sample - loss: 0.2576 - accuracy: 0.8983 - val_loss: 0.2862 - val_accuracy: 0.8859 - lr: 0.0010\n",
            "Epoch 59/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.2526 - accuracy: 0.8988\n",
            "Epoch 00059: val_loss improved from 0.28506 to 0.28096, saving model to logs/DeepConvNet_out_weights.h5\n",
            "33280/33280 [==============================] - 11s 319us/sample - loss: 0.2526 - accuracy: 0.8988 - val_loss: 0.2810 - val_accuracy: 0.8882 - lr: 0.0010\n",
            "Epoch 60/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.2559 - accuracy: 0.8993\n",
            "Epoch 00060: val_loss did not improve from 0.28096\n",
            "33280/33280 [==============================] - 10s 309us/sample - loss: 0.2559 - accuracy: 0.8993 - val_loss: 0.2813 - val_accuracy: 0.8874 - lr: 0.0010\n",
            "Epoch 61/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.2564 - accuracy: 0.8972\n",
            "Epoch 00061: val_loss did not improve from 0.28096\n",
            "33280/33280 [==============================] - 10s 308us/sample - loss: 0.2564 - accuracy: 0.8972 - val_loss: 0.2810 - val_accuracy: 0.8841 - lr: 0.0010\n",
            "Epoch 62/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.2515 - accuracy: 0.8996\n",
            "Epoch 00062: val_loss improved from 0.28096 to 0.27517, saving model to logs/DeepConvNet_out_weights.h5\n",
            "33280/33280 [==============================] - 10s 312us/sample - loss: 0.2515 - accuracy: 0.8996 - val_loss: 0.2752 - val_accuracy: 0.8897 - lr: 0.0010\n",
            "Epoch 63/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.2454 - accuracy: 0.9033\n",
            "Epoch 00063: val_loss did not improve from 0.27517\n",
            "33280/33280 [==============================] - 10s 306us/sample - loss: 0.2454 - accuracy: 0.9033 - val_loss: 0.2851 - val_accuracy: 0.8846 - lr: 0.0010\n",
            "Epoch 64/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.2521 - accuracy: 0.8989\n",
            "Epoch 00064: val_loss did not improve from 0.27517\n",
            "33280/33280 [==============================] - 10s 303us/sample - loss: 0.2521 - accuracy: 0.8989 - val_loss: 0.2762 - val_accuracy: 0.8889 - lr: 0.0010\n",
            "Epoch 65/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.2466 - accuracy: 0.9018\n",
            "Epoch 00065: val_loss improved from 0.27517 to 0.27235, saving model to logs/DeepConvNet_out_weights.h5\n",
            "33280/33280 [==============================] - 10s 307us/sample - loss: 0.2466 - accuracy: 0.9018 - val_loss: 0.2723 - val_accuracy: 0.8930 - lr: 0.0010\n",
            "Epoch 66/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.2448 - accuracy: 0.9014\n",
            "Epoch 00066: val_loss did not improve from 0.27235\n",
            "33280/33280 [==============================] - 10s 305us/sample - loss: 0.2448 - accuracy: 0.9014 - val_loss: 0.2770 - val_accuracy: 0.8873 - lr: 0.0010\n",
            "Epoch 67/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.2490 - accuracy: 0.8995\n",
            "Epoch 00067: val_loss did not improve from 0.27235\n",
            "33280/33280 [==============================] - 10s 309us/sample - loss: 0.2490 - accuracy: 0.8995 - val_loss: 0.2778 - val_accuracy: 0.8858 - lr: 0.0010\n",
            "Epoch 68/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.2482 - accuracy: 0.9003\n",
            "Epoch 00068: val_loss did not improve from 0.27235\n",
            "33280/33280 [==============================] - 10s 304us/sample - loss: 0.2482 - accuracy: 0.9003 - val_loss: 0.2833 - val_accuracy: 0.8883 - lr: 0.0010\n",
            "Epoch 69/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.2416 - accuracy: 0.9038\n",
            "Epoch 00069: val_loss did not improve from 0.27235\n",
            "33280/33280 [==============================] - 10s 304us/sample - loss: 0.2416 - accuracy: 0.9038 - val_loss: 0.2724 - val_accuracy: 0.8931 - lr: 0.0010\n",
            "Epoch 70/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.2390 - accuracy: 0.9042\n",
            "Epoch 00070: val_loss improved from 0.27235 to 0.26863, saving model to logs/DeepConvNet_out_weights.h5\n",
            "33280/33280 [==============================] - 10s 311us/sample - loss: 0.2390 - accuracy: 0.9042 - val_loss: 0.2686 - val_accuracy: 0.8945 - lr: 0.0010\n",
            "Epoch 71/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.2447 - accuracy: 0.9019\n",
            "Epoch 00071: val_loss did not improve from 0.26863\n",
            "33280/33280 [==============================] - 10s 307us/sample - loss: 0.2447 - accuracy: 0.9019 - val_loss: 0.2791 - val_accuracy: 0.8875 - lr: 0.0010\n",
            "Epoch 72/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.2346 - accuracy: 0.9074\n",
            "Epoch 00072: val_loss did not improve from 0.26863\n",
            "33280/33280 [==============================] - 10s 309us/sample - loss: 0.2346 - accuracy: 0.9074 - val_loss: 0.2718 - val_accuracy: 0.8904 - lr: 0.0010\n",
            "Epoch 73/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.2338 - accuracy: 0.9072\n",
            "Epoch 00073: val_loss improved from 0.26863 to 0.26653, saving model to logs/DeepConvNet_out_weights.h5\n",
            "33280/33280 [==============================] - 10s 306us/sample - loss: 0.2338 - accuracy: 0.9072 - val_loss: 0.2665 - val_accuracy: 0.8942 - lr: 0.0010\n",
            "Epoch 74/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.2346 - accuracy: 0.9049\n",
            "Epoch 00074: val_loss improved from 0.26653 to 0.26505, saving model to logs/DeepConvNet_out_weights.h5\n",
            "33280/33280 [==============================] - 10s 308us/sample - loss: 0.2346 - accuracy: 0.9049 - val_loss: 0.2650 - val_accuracy: 0.8921 - lr: 0.0010\n",
            "Epoch 75/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.2326 - accuracy: 0.9062\n",
            "Epoch 00075: val_loss did not improve from 0.26505\n",
            "33280/33280 [==============================] - 10s 307us/sample - loss: 0.2326 - accuracy: 0.9062 - val_loss: 0.2732 - val_accuracy: 0.8922 - lr: 0.0010\n",
            "Epoch 76/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.2322 - accuracy: 0.9075\n",
            "Epoch 00076: val_loss did not improve from 0.26505\n",
            "33280/33280 [==============================] - 10s 309us/sample - loss: 0.2322 - accuracy: 0.9075 - val_loss: 0.2702 - val_accuracy: 0.8915 - lr: 0.0010\n",
            "Epoch 77/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.2369 - accuracy: 0.9064\n",
            "Epoch 00077: val_loss did not improve from 0.26505\n",
            "33280/33280 [==============================] - 10s 309us/sample - loss: 0.2369 - accuracy: 0.9064 - val_loss: 0.2714 - val_accuracy: 0.8909 - lr: 0.0010\n",
            "Epoch 78/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.2343 - accuracy: 0.9058\n",
            "Epoch 00078: val_loss did not improve from 0.26505\n",
            "33280/33280 [==============================] - 10s 309us/sample - loss: 0.2343 - accuracy: 0.9058 - val_loss: 0.2679 - val_accuracy: 0.8918 - lr: 0.0010\n",
            "Epoch 79/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.2342 - accuracy: 0.9097\n",
            "Epoch 00079: val_loss improved from 0.26505 to 0.26401, saving model to logs/DeepConvNet_out_weights.h5\n",
            "33280/33280 [==============================] - 10s 311us/sample - loss: 0.2342 - accuracy: 0.9097 - val_loss: 0.2640 - val_accuracy: 0.8925 - lr: 0.0010\n",
            "Epoch 80/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.2349 - accuracy: 0.9067\n",
            "Epoch 00080: val_loss did not improve from 0.26401\n",
            "33280/33280 [==============================] - 10s 309us/sample - loss: 0.2349 - accuracy: 0.9067 - val_loss: 0.2758 - val_accuracy: 0.8909 - lr: 0.0010\n",
            "Epoch 81/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.2272 - accuracy: 0.9109\n",
            "Epoch 00081: val_loss did not improve from 0.26401\n",
            "33280/33280 [==============================] - 10s 309us/sample - loss: 0.2272 - accuracy: 0.9109 - val_loss: 0.2641 - val_accuracy: 0.8969 - lr: 0.0010\n",
            "Epoch 82/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.2332 - accuracy: 0.9100\n",
            "Epoch 00082: val_loss improved from 0.26401 to 0.25898, saving model to logs/DeepConvNet_out_weights.h5\n",
            "33280/33280 [==============================] - 10s 312us/sample - loss: 0.2332 - accuracy: 0.9100 - val_loss: 0.2590 - val_accuracy: 0.8945 - lr: 0.0010\n",
            "Epoch 83/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.2289 - accuracy: 0.9092\n",
            "Epoch 00083: val_loss did not improve from 0.25898\n",
            "33280/33280 [==============================] - 10s 308us/sample - loss: 0.2289 - accuracy: 0.9092 - val_loss: 0.2686 - val_accuracy: 0.8952 - lr: 0.0010\n",
            "Epoch 84/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.2262 - accuracy: 0.9114\n",
            "Epoch 00084: val_loss did not improve from 0.25898\n",
            "33280/33280 [==============================] - 10s 307us/sample - loss: 0.2262 - accuracy: 0.9114 - val_loss: 0.2620 - val_accuracy: 0.8948 - lr: 0.0010\n",
            "Epoch 85/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.2200 - accuracy: 0.9135\n",
            "Epoch 00085: val_loss did not improve from 0.25898\n",
            "33280/33280 [==============================] - 10s 309us/sample - loss: 0.2200 - accuracy: 0.9135 - val_loss: 0.2600 - val_accuracy: 0.8945 - lr: 0.0010\n",
            "Epoch 86/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.2184 - accuracy: 0.9143\n",
            "Epoch 00086: val_loss did not improve from 0.25898\n",
            "33280/33280 [==============================] - 10s 309us/sample - loss: 0.2184 - accuracy: 0.9143 - val_loss: 0.2617 - val_accuracy: 0.8946 - lr: 0.0010\n",
            "Epoch 87/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.2254 - accuracy: 0.9098\n",
            "Epoch 00087: val_loss did not improve from 0.25898\n",
            "33280/33280 [==============================] - 10s 309us/sample - loss: 0.2254 - accuracy: 0.9098 - val_loss: 0.2660 - val_accuracy: 0.8931 - lr: 0.0010\n",
            "Epoch 88/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.2259 - accuracy: 0.9108\n",
            "Epoch 00088: val_loss improved from 0.25898 to 0.25256, saving model to logs/DeepConvNet_out_weights.h5\n",
            "33280/33280 [==============================] - 10s 311us/sample - loss: 0.2259 - accuracy: 0.9108 - val_loss: 0.2526 - val_accuracy: 0.8995 - lr: 0.0010\n",
            "Epoch 89/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.2261 - accuracy: 0.9096\n",
            "Epoch 00089: val_loss did not improve from 0.25256\n",
            "33280/33280 [==============================] - 10s 309us/sample - loss: 0.2261 - accuracy: 0.9096 - val_loss: 0.2642 - val_accuracy: 0.8940 - lr: 0.0010\n",
            "Epoch 90/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.2212 - accuracy: 0.9133\n",
            "Epoch 00090: val_loss did not improve from 0.25256\n",
            "33280/33280 [==============================] - 10s 308us/sample - loss: 0.2212 - accuracy: 0.9133 - val_loss: 0.2585 - val_accuracy: 0.8989 - lr: 0.0010\n",
            "Epoch 91/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.2239 - accuracy: 0.9113\n",
            "Epoch 00091: val_loss did not improve from 0.25256\n",
            "33280/33280 [==============================] - 10s 314us/sample - loss: 0.2239 - accuracy: 0.9113 - val_loss: 0.2536 - val_accuracy: 0.8984 - lr: 0.0010\n",
            "Epoch 92/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.2161 - accuracy: 0.9167\n",
            "Epoch 00092: val_loss did not improve from 0.25256\n",
            "33280/33280 [==============================] - 10s 310us/sample - loss: 0.2161 - accuracy: 0.9167 - val_loss: 0.2562 - val_accuracy: 0.8963 - lr: 0.0010\n",
            "Epoch 93/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.2129 - accuracy: 0.9166\n",
            "Epoch 00093: val_loss improved from 0.25256 to 0.25201, saving model to logs/DeepConvNet_out_weights.h5\n",
            "33280/33280 [==============================] - 10s 310us/sample - loss: 0.2129 - accuracy: 0.9166 - val_loss: 0.2520 - val_accuracy: 0.8976 - lr: 0.0010\n",
            "Epoch 94/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.2173 - accuracy: 0.9147\n",
            "Epoch 00094: val_loss did not improve from 0.25201\n",
            "33280/33280 [==============================] - 10s 310us/sample - loss: 0.2173 - accuracy: 0.9147 - val_loss: 0.2613 - val_accuracy: 0.8960 - lr: 0.0010\n",
            "Epoch 95/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.2179 - accuracy: 0.9142\n",
            "Epoch 00095: val_loss did not improve from 0.25201\n",
            "33280/33280 [==============================] - 10s 311us/sample - loss: 0.2179 - accuracy: 0.9142 - val_loss: 0.2606 - val_accuracy: 0.8980 - lr: 0.0010\n",
            "Epoch 96/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.2150 - accuracy: 0.9167\n",
            "Epoch 00096: val_loss did not improve from 0.25201\n",
            "33280/33280 [==============================] - 10s 308us/sample - loss: 0.2150 - accuracy: 0.9167 - val_loss: 0.2524 - val_accuracy: 0.8998 - lr: 0.0010\n",
            "Epoch 97/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.2142 - accuracy: 0.9148\n",
            "Epoch 00097: val_loss did not improve from 0.25201\n",
            "33280/33280 [==============================] - 10s 310us/sample - loss: 0.2142 - accuracy: 0.9148 - val_loss: 0.2724 - val_accuracy: 0.8909 - lr: 0.0010\n",
            "Epoch 98/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.2077 - accuracy: 0.9194\n",
            "Epoch 00098: val_loss did not improve from 0.25201\n",
            "33280/33280 [==============================] - 10s 309us/sample - loss: 0.2077 - accuracy: 0.9194 - val_loss: 0.2532 - val_accuracy: 0.8980 - lr: 0.0010\n",
            "Epoch 99/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.2123 - accuracy: 0.9164\n",
            "Epoch 00099: val_loss did not improve from 0.25201\n",
            "33280/33280 [==============================] - 10s 308us/sample - loss: 0.2123 - accuracy: 0.9164 - val_loss: 0.2556 - val_accuracy: 0.8969 - lr: 0.0010\n",
            "Epoch 100/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.2174 - accuracy: 0.9130\n",
            "Epoch 00100: val_loss did not improve from 0.25201\n",
            "33280/33280 [==============================] - 10s 309us/sample - loss: 0.2174 - accuracy: 0.9130 - val_loss: 0.2616 - val_accuracy: 0.8994 - lr: 0.0010\n",
            "Epoch 101/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.2210 - accuracy: 0.9114\n",
            "Epoch 00101: val_loss improved from 0.25201 to 0.24894, saving model to logs/DeepConvNet_out_weights.h5\n",
            "33280/33280 [==============================] - 10s 311us/sample - loss: 0.2210 - accuracy: 0.9114 - val_loss: 0.2489 - val_accuracy: 0.9018 - lr: 0.0010\n",
            "Epoch 102/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.2102 - accuracy: 0.9174\n",
            "Epoch 00102: val_loss did not improve from 0.24894\n",
            "33280/33280 [==============================] - 10s 308us/sample - loss: 0.2102 - accuracy: 0.9174 - val_loss: 0.2587 - val_accuracy: 0.8987 - lr: 0.0010\n",
            "Epoch 103/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.2106 - accuracy: 0.9175\n",
            "Epoch 00103: val_loss did not improve from 0.24894\n",
            "33280/33280 [==============================] - 10s 308us/sample - loss: 0.2106 - accuracy: 0.9175 - val_loss: 0.2637 - val_accuracy: 0.8933 - lr: 0.0010\n",
            "Epoch 104/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.2044 - accuracy: 0.9203\n",
            "Epoch 00104: val_loss did not improve from 0.24894\n",
            "33280/33280 [==============================] - 10s 308us/sample - loss: 0.2044 - accuracy: 0.9203 - val_loss: 0.2566 - val_accuracy: 0.9000 - lr: 0.0010\n",
            "Epoch 105/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.2128 - accuracy: 0.9152\n",
            "Epoch 00105: val_loss did not improve from 0.24894\n",
            "33280/33280 [==============================] - 10s 308us/sample - loss: 0.2128 - accuracy: 0.9152 - val_loss: 0.2615 - val_accuracy: 0.8964 - lr: 0.0010\n",
            "Epoch 106/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.2102 - accuracy: 0.9176\n",
            "Epoch 00106: val_loss did not improve from 0.24894\n",
            "33280/33280 [==============================] - 10s 310us/sample - loss: 0.2102 - accuracy: 0.9176 - val_loss: 0.2499 - val_accuracy: 0.8974 - lr: 0.0010\n",
            "Epoch 107/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.2048 - accuracy: 0.9209\n",
            "Epoch 00107: val_loss did not improve from 0.24894\n",
            "33280/33280 [==============================] - 10s 308us/sample - loss: 0.2048 - accuracy: 0.9209 - val_loss: 0.2498 - val_accuracy: 0.8987 - lr: 0.0010\n",
            "Epoch 108/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.2087 - accuracy: 0.9182\n",
            "Epoch 00108: val_loss did not improve from 0.24894\n",
            "33280/33280 [==============================] - 10s 307us/sample - loss: 0.2087 - accuracy: 0.9182 - val_loss: 0.2503 - val_accuracy: 0.8977 - lr: 0.0010\n",
            "Epoch 109/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.2087 - accuracy: 0.9177\n",
            "Epoch 00109: val_loss did not improve from 0.24894\n",
            "33280/33280 [==============================] - 10s 308us/sample - loss: 0.2087 - accuracy: 0.9177 - val_loss: 0.2531 - val_accuracy: 0.8976 - lr: 0.0010\n",
            "Epoch 110/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.2079 - accuracy: 0.9179\n",
            "Epoch 00110: val_loss did not improve from 0.24894\n",
            "33280/33280 [==============================] - 10s 306us/sample - loss: 0.2079 - accuracy: 0.9179 - val_loss: 0.2621 - val_accuracy: 0.8934 - lr: 0.0010\n",
            "Epoch 111/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.2074 - accuracy: 0.9179\n",
            "Epoch 00111: val_loss did not improve from 0.24894\n",
            "\n",
            "Epoch 00111: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "33280/33280 [==============================] - 10s 310us/sample - loss: 0.2074 - accuracy: 0.9179 - val_loss: 0.2503 - val_accuracy: 0.8958 - lr: 0.0010\n",
            "Epoch 112/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.1862 - accuracy: 0.9271\n",
            "Epoch 00112: val_loss improved from 0.24894 to 0.23530, saving model to logs/DeepConvNet_out_weights.h5\n",
            "33280/33280 [==============================] - 10s 306us/sample - loss: 0.1862 - accuracy: 0.9271 - val_loss: 0.2353 - val_accuracy: 0.9052 - lr: 2.5000e-04\n",
            "Epoch 113/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.1766 - accuracy: 0.9320\n",
            "Epoch 00113: val_loss improved from 0.23530 to 0.23350, saving model to logs/DeepConvNet_out_weights.h5\n",
            "33280/33280 [==============================] - 10s 307us/sample - loss: 0.1766 - accuracy: 0.9320 - val_loss: 0.2335 - val_accuracy: 0.9071 - lr: 2.5000e-04\n",
            "Epoch 114/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.1757 - accuracy: 0.9329\n",
            "Epoch 00114: val_loss improved from 0.23350 to 0.23331, saving model to logs/DeepConvNet_out_weights.h5\n",
            "33280/33280 [==============================] - 10s 304us/sample - loss: 0.1757 - accuracy: 0.9329 - val_loss: 0.2333 - val_accuracy: 0.9072 - lr: 2.5000e-04\n",
            "Epoch 115/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.1731 - accuracy: 0.9342\n",
            "Epoch 00115: val_loss improved from 0.23331 to 0.23122, saving model to logs/DeepConvNet_out_weights.h5\n",
            "33280/33280 [==============================] - 10s 310us/sample - loss: 0.1731 - accuracy: 0.9342 - val_loss: 0.2312 - val_accuracy: 0.9075 - lr: 2.5000e-04\n",
            "Epoch 116/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.1742 - accuracy: 0.9336\n",
            "Epoch 00116: val_loss did not improve from 0.23122\n",
            "33280/33280 [==============================] - 10s 308us/sample - loss: 0.1742 - accuracy: 0.9336 - val_loss: 0.2328 - val_accuracy: 0.9066 - lr: 2.5000e-04\n",
            "Epoch 117/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.1704 - accuracy: 0.9358\n",
            "Epoch 00117: val_loss did not improve from 0.23122\n",
            "33280/33280 [==============================] - 10s 309us/sample - loss: 0.1704 - accuracy: 0.9358 - val_loss: 0.2338 - val_accuracy: 0.9072 - lr: 2.5000e-04\n",
            "Epoch 118/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.1688 - accuracy: 0.9354\n",
            "Epoch 00118: val_loss improved from 0.23122 to 0.22751, saving model to logs/DeepConvNet_out_weights.h5\n",
            "33280/33280 [==============================] - 10s 311us/sample - loss: 0.1688 - accuracy: 0.9354 - val_loss: 0.2275 - val_accuracy: 0.9107 - lr: 2.5000e-04\n",
            "Epoch 119/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.1684 - accuracy: 0.9357\n",
            "Epoch 00119: val_loss did not improve from 0.22751\n",
            "33280/33280 [==============================] - 10s 309us/sample - loss: 0.1684 - accuracy: 0.9357 - val_loss: 0.2310 - val_accuracy: 0.9078 - lr: 2.5000e-04\n",
            "Epoch 120/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.1665 - accuracy: 0.9363\n",
            "Epoch 00120: val_loss did not improve from 0.22751\n",
            "33280/33280 [==============================] - 10s 309us/sample - loss: 0.1665 - accuracy: 0.9363 - val_loss: 0.2311 - val_accuracy: 0.9076 - lr: 2.5000e-04\n",
            "Epoch 121/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.1668 - accuracy: 0.9371\n",
            "Epoch 00121: val_loss did not improve from 0.22751\n",
            "33280/33280 [==============================] - 10s 310us/sample - loss: 0.1668 - accuracy: 0.9371 - val_loss: 0.2322 - val_accuracy: 0.9072 - lr: 2.5000e-04\n",
            "Epoch 122/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.1679 - accuracy: 0.9351\n",
            "Epoch 00122: val_loss did not improve from 0.22751\n",
            "33280/33280 [==============================] - 10s 310us/sample - loss: 0.1679 - accuracy: 0.9351 - val_loss: 0.2300 - val_accuracy: 0.9099 - lr: 2.5000e-04\n",
            "Epoch 123/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.1686 - accuracy: 0.9344\n",
            "Epoch 00123: val_loss did not improve from 0.22751\n",
            "33280/33280 [==============================] - 10s 309us/sample - loss: 0.1686 - accuracy: 0.9344 - val_loss: 0.2301 - val_accuracy: 0.9072 - lr: 2.5000e-04\n",
            "Epoch 124/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.1645 - accuracy: 0.9379\n",
            "Epoch 00124: val_loss did not improve from 0.22751\n",
            "33280/33280 [==============================] - 10s 308us/sample - loss: 0.1645 - accuracy: 0.9379 - val_loss: 0.2308 - val_accuracy: 0.9064 - lr: 2.5000e-04\n",
            "Epoch 125/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.1659 - accuracy: 0.9368\n",
            "Epoch 00125: val_loss did not improve from 0.22751\n",
            "33280/33280 [==============================] - 10s 306us/sample - loss: 0.1659 - accuracy: 0.9368 - val_loss: 0.2281 - val_accuracy: 0.9066 - lr: 2.5000e-04\n",
            "Epoch 126/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.1665 - accuracy: 0.9377\n",
            "Epoch 00126: val_loss did not improve from 0.22751\n",
            "33280/33280 [==============================] - 10s 307us/sample - loss: 0.1665 - accuracy: 0.9377 - val_loss: 0.2292 - val_accuracy: 0.9088 - lr: 2.5000e-04\n",
            "Epoch 127/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.1660 - accuracy: 0.9373\n",
            "Epoch 00127: val_loss improved from 0.22751 to 0.22735, saving model to logs/DeepConvNet_out_weights.h5\n",
            "33280/33280 [==============================] - 10s 309us/sample - loss: 0.1660 - accuracy: 0.9373 - val_loss: 0.2274 - val_accuracy: 0.9103 - lr: 2.5000e-04\n",
            "Epoch 128/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.1657 - accuracy: 0.9380\n",
            "Epoch 00128: val_loss did not improve from 0.22735\n",
            "33280/33280 [==============================] - 10s 308us/sample - loss: 0.1657 - accuracy: 0.9380 - val_loss: 0.2309 - val_accuracy: 0.9071 - lr: 2.5000e-04\n",
            "Epoch 129/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.1657 - accuracy: 0.9369\n",
            "Epoch 00129: val_loss did not improve from 0.22735\n",
            "33280/33280 [==============================] - 10s 306us/sample - loss: 0.1657 - accuracy: 0.9369 - val_loss: 0.2314 - val_accuracy: 0.9065 - lr: 2.5000e-04\n",
            "Epoch 130/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.1624 - accuracy: 0.9382\n",
            "Epoch 00130: val_loss did not improve from 0.22735\n",
            "33280/33280 [==============================] - 10s 309us/sample - loss: 0.1624 - accuracy: 0.9382 - val_loss: 0.2294 - val_accuracy: 0.9067 - lr: 2.5000e-04\n",
            "Epoch 131/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.1679 - accuracy: 0.9355\n",
            "Epoch 00131: val_loss improved from 0.22735 to 0.22297, saving model to logs/DeepConvNet_out_weights.h5\n",
            "33280/33280 [==============================] - 10s 309us/sample - loss: 0.1679 - accuracy: 0.9355 - val_loss: 0.2230 - val_accuracy: 0.9105 - lr: 2.5000e-04\n",
            "Epoch 132/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.1632 - accuracy: 0.9393\n",
            "Epoch 00132: val_loss did not improve from 0.22297\n",
            "33280/33280 [==============================] - 10s 307us/sample - loss: 0.1632 - accuracy: 0.9393 - val_loss: 0.2263 - val_accuracy: 0.9087 - lr: 2.5000e-04\n",
            "Epoch 133/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.1620 - accuracy: 0.9380\n",
            "Epoch 00133: val_loss did not improve from 0.22297\n",
            "33280/33280 [==============================] - 10s 309us/sample - loss: 0.1620 - accuracy: 0.9380 - val_loss: 0.2294 - val_accuracy: 0.9060 - lr: 2.5000e-04\n",
            "Epoch 134/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.1644 - accuracy: 0.9378\n",
            "Epoch 00134: val_loss did not improve from 0.22297\n",
            "33280/33280 [==============================] - 10s 307us/sample - loss: 0.1644 - accuracy: 0.9378 - val_loss: 0.2293 - val_accuracy: 0.9062 - lr: 2.5000e-04\n",
            "Epoch 135/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.1658 - accuracy: 0.9367\n",
            "Epoch 00135: val_loss did not improve from 0.22297\n",
            "33280/33280 [==============================] - 10s 305us/sample - loss: 0.1658 - accuracy: 0.9367 - val_loss: 0.2283 - val_accuracy: 0.9087 - lr: 2.5000e-04\n",
            "Epoch 136/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.1634 - accuracy: 0.9389\n",
            "Epoch 00136: val_loss did not improve from 0.22297\n",
            "33280/33280 [==============================] - 10s 307us/sample - loss: 0.1634 - accuracy: 0.9389 - val_loss: 0.2257 - val_accuracy: 0.9093 - lr: 2.5000e-04\n",
            "Epoch 137/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.1645 - accuracy: 0.9374\n",
            "Epoch 00137: val_loss did not improve from 0.22297\n",
            "33280/33280 [==============================] - 10s 307us/sample - loss: 0.1645 - accuracy: 0.9374 - val_loss: 0.2289 - val_accuracy: 0.9062 - lr: 2.5000e-04\n",
            "Epoch 138/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.1615 - accuracy: 0.9394\n",
            "Epoch 00138: val_loss did not improve from 0.22297\n",
            "33280/33280 [==============================] - 10s 308us/sample - loss: 0.1615 - accuracy: 0.9394 - val_loss: 0.2282 - val_accuracy: 0.9091 - lr: 2.5000e-04\n",
            "Epoch 139/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.1633 - accuracy: 0.9384\n",
            "Epoch 00139: val_loss did not improve from 0.22297\n",
            "33280/33280 [==============================] - 10s 309us/sample - loss: 0.1633 - accuracy: 0.9384 - val_loss: 0.2268 - val_accuracy: 0.9102 - lr: 2.5000e-04\n",
            "Epoch 140/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.1607 - accuracy: 0.9396\n",
            "Epoch 00140: val_loss did not improve from 0.22297\n",
            "33280/33280 [==============================] - 10s 308us/sample - loss: 0.1607 - accuracy: 0.9396 - val_loss: 0.2299 - val_accuracy: 0.9069 - lr: 2.5000e-04\n",
            "Epoch 141/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.1623 - accuracy: 0.9372\n",
            "Epoch 00141: val_loss did not improve from 0.22297\n",
            "\n",
            "Epoch 00141: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "33280/33280 [==============================] - 10s 306us/sample - loss: 0.1623 - accuracy: 0.9372 - val_loss: 0.2278 - val_accuracy: 0.9085 - lr: 2.5000e-04\n",
            "Epoch 142/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.1589 - accuracy: 0.9397\n",
            "Epoch 00142: val_loss did not improve from 0.22297\n",
            "33280/33280 [==============================] - 10s 307us/sample - loss: 0.1589 - accuracy: 0.9397 - val_loss: 0.2236 - val_accuracy: 0.9109 - lr: 6.2500e-05\n",
            "Epoch 143/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.1560 - accuracy: 0.9410\n",
            "Epoch 00143: val_loss improved from 0.22297 to 0.22181, saving model to logs/DeepConvNet_out_weights.h5\n",
            "33280/33280 [==============================] - 10s 309us/sample - loss: 0.1560 - accuracy: 0.9410 - val_loss: 0.2218 - val_accuracy: 0.9111 - lr: 6.2500e-05\n",
            "Epoch 144/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.1519 - accuracy: 0.9424\n",
            "Epoch 00144: val_loss did not improve from 0.22181\n",
            "33280/33280 [==============================] - 10s 302us/sample - loss: 0.1519 - accuracy: 0.9424 - val_loss: 0.2222 - val_accuracy: 0.9111 - lr: 6.2500e-05\n",
            "Epoch 145/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.1527 - accuracy: 0.9430\n",
            "Epoch 00145: val_loss improved from 0.22181 to 0.22154, saving model to logs/DeepConvNet_out_weights.h5\n",
            "33280/33280 [==============================] - 10s 304us/sample - loss: 0.1527 - accuracy: 0.9430 - val_loss: 0.2215 - val_accuracy: 0.9117 - lr: 6.2500e-05\n",
            "Epoch 146/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.1570 - accuracy: 0.9401\n",
            "Epoch 00146: val_loss improved from 0.22154 to 0.22117, saving model to logs/DeepConvNet_out_weights.h5\n",
            "33280/33280 [==============================] - 10s 304us/sample - loss: 0.1570 - accuracy: 0.9401 - val_loss: 0.2212 - val_accuracy: 0.9135 - lr: 6.2500e-05\n",
            "Epoch 147/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.1598 - accuracy: 0.9393\n",
            "Epoch 00147: val_loss did not improve from 0.22117\n",
            "33280/33280 [==============================] - 10s 302us/sample - loss: 0.1598 - accuracy: 0.9393 - val_loss: 0.2221 - val_accuracy: 0.9125 - lr: 6.2500e-05\n",
            "Epoch 148/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.1521 - accuracy: 0.9434\n",
            "Epoch 00148: val_loss did not improve from 0.22117\n",
            "33280/33280 [==============================] - 10s 303us/sample - loss: 0.1521 - accuracy: 0.9434 - val_loss: 0.2213 - val_accuracy: 0.9103 - lr: 6.2500e-05\n",
            "Epoch 149/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.1569 - accuracy: 0.9410\n",
            "Epoch 00149: val_loss did not improve from 0.22117\n",
            "33280/33280 [==============================] - 10s 304us/sample - loss: 0.1569 - accuracy: 0.9410 - val_loss: 0.2223 - val_accuracy: 0.9113 - lr: 6.2500e-05\n",
            "Epoch 150/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.1522 - accuracy: 0.9438\n",
            "Epoch 00150: val_loss did not improve from 0.22117\n",
            "33280/33280 [==============================] - 10s 307us/sample - loss: 0.1522 - accuracy: 0.9438 - val_loss: 0.2223 - val_accuracy: 0.9113 - lr: 6.2500e-05\n",
            "Epoch 151/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.1550 - accuracy: 0.9416\n",
            "Epoch 00151: val_loss did not improve from 0.22117\n",
            "33280/33280 [==============================] - 10s 312us/sample - loss: 0.1550 - accuracy: 0.9416 - val_loss: 0.2219 - val_accuracy: 0.9105 - lr: 6.2500e-05\n",
            "Epoch 152/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.1538 - accuracy: 0.9434\n",
            "Epoch 00152: val_loss did not improve from 0.22117\n",
            "33280/33280 [==============================] - 10s 309us/sample - loss: 0.1538 - accuracy: 0.9434 - val_loss: 0.2212 - val_accuracy: 0.9114 - lr: 6.2500e-05\n",
            "Epoch 153/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.1534 - accuracy: 0.9436\n",
            "Epoch 00153: val_loss improved from 0.22117 to 0.22082, saving model to logs/DeepConvNet_out_weights.h5\n",
            "33280/33280 [==============================] - 10s 313us/sample - loss: 0.1534 - accuracy: 0.9436 - val_loss: 0.2208 - val_accuracy: 0.9138 - lr: 6.2500e-05\n",
            "Epoch 154/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.1519 - accuracy: 0.9432\n",
            "Epoch 00154: val_loss did not improve from 0.22082\n",
            "33280/33280 [==============================] - 10s 309us/sample - loss: 0.1519 - accuracy: 0.9432 - val_loss: 0.2212 - val_accuracy: 0.9130 - lr: 6.2500e-05\n",
            "Epoch 155/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.1513 - accuracy: 0.9446\n",
            "Epoch 00155: val_loss improved from 0.22082 to 0.21938, saving model to logs/DeepConvNet_out_weights.h5\n",
            "33280/33280 [==============================] - 10s 311us/sample - loss: 0.1513 - accuracy: 0.9446 - val_loss: 0.2194 - val_accuracy: 0.9126 - lr: 6.2500e-05\n",
            "Epoch 156/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.1514 - accuracy: 0.9436\n",
            "Epoch 00156: val_loss did not improve from 0.21938\n",
            "33280/33280 [==============================] - 10s 309us/sample - loss: 0.1514 - accuracy: 0.9436 - val_loss: 0.2219 - val_accuracy: 0.9119 - lr: 6.2500e-05\n",
            "Epoch 157/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.1519 - accuracy: 0.9420\n",
            "Epoch 00157: val_loss did not improve from 0.21938\n",
            "33280/33280 [==============================] - 10s 309us/sample - loss: 0.1519 - accuracy: 0.9420 - val_loss: 0.2223 - val_accuracy: 0.9106 - lr: 6.2500e-05\n",
            "Epoch 158/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.1524 - accuracy: 0.9424\n",
            "Epoch 00158: val_loss did not improve from 0.21938\n",
            "33280/33280 [==============================] - 10s 307us/sample - loss: 0.1524 - accuracy: 0.9424 - val_loss: 0.2212 - val_accuracy: 0.9112 - lr: 6.2500e-05\n",
            "Epoch 159/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.1548 - accuracy: 0.9403\n",
            "Epoch 00159: val_loss did not improve from 0.21938\n",
            "33280/33280 [==============================] - 10s 307us/sample - loss: 0.1548 - accuracy: 0.9403 - val_loss: 0.2209 - val_accuracy: 0.9113 - lr: 6.2500e-05\n",
            "Epoch 160/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.1520 - accuracy: 0.9438\n",
            "Epoch 00160: val_loss did not improve from 0.21938\n",
            "33280/33280 [==============================] - 10s 306us/sample - loss: 0.1520 - accuracy: 0.9438 - val_loss: 0.2224 - val_accuracy: 0.9107 - lr: 6.2500e-05\n",
            "Epoch 161/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.1522 - accuracy: 0.9420\n",
            "Epoch 00161: val_loss did not improve from 0.21938\n",
            "33280/33280 [==============================] - 10s 308us/sample - loss: 0.1522 - accuracy: 0.9420 - val_loss: 0.2208 - val_accuracy: 0.9125 - lr: 6.2500e-05\n",
            "Epoch 162/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.1512 - accuracy: 0.9436\n",
            "Epoch 00162: val_loss did not improve from 0.21938\n",
            "33280/33280 [==============================] - 10s 309us/sample - loss: 0.1512 - accuracy: 0.9436 - val_loss: 0.2206 - val_accuracy: 0.9117 - lr: 6.2500e-05\n",
            "Epoch 163/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.1511 - accuracy: 0.9434\n",
            "Epoch 00163: val_loss did not improve from 0.21938\n",
            "33280/33280 [==============================] - 10s 308us/sample - loss: 0.1511 - accuracy: 0.9434 - val_loss: 0.2221 - val_accuracy: 0.9111 - lr: 6.2500e-05\n",
            "Epoch 164/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.1509 - accuracy: 0.9437\n",
            "Epoch 00164: val_loss did not improve from 0.21938\n",
            "33280/33280 [==============================] - 10s 308us/sample - loss: 0.1509 - accuracy: 0.9437 - val_loss: 0.2222 - val_accuracy: 0.9112 - lr: 6.2500e-05\n",
            "Epoch 165/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.1520 - accuracy: 0.9432\n",
            "Epoch 00165: val_loss did not improve from 0.21938\n",
            "\n",
            "Epoch 00165: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "33280/33280 [==============================] - 10s 308us/sample - loss: 0.1520 - accuracy: 0.9432 - val_loss: 0.2210 - val_accuracy: 0.9112 - lr: 6.2500e-05\n",
            "Epoch 166/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.1511 - accuracy: 0.9419\n",
            "Epoch 00166: val_loss did not improve from 0.21938\n",
            "33280/33280 [==============================] - 10s 307us/sample - loss: 0.1511 - accuracy: 0.9419 - val_loss: 0.2216 - val_accuracy: 0.9109 - lr: 1.5625e-05\n",
            "Epoch 167/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.1476 - accuracy: 0.9447\n",
            "Epoch 00167: val_loss did not improve from 0.21938\n",
            "33280/33280 [==============================] - 10s 308us/sample - loss: 0.1476 - accuracy: 0.9447 - val_loss: 0.2197 - val_accuracy: 0.9125 - lr: 1.5625e-05\n",
            "Epoch 168/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.1491 - accuracy: 0.9441\n",
            "Epoch 00168: val_loss did not improve from 0.21938\n",
            "33280/33280 [==============================] - 10s 308us/sample - loss: 0.1491 - accuracy: 0.9441 - val_loss: 0.2208 - val_accuracy: 0.9109 - lr: 1.5625e-05\n",
            "Epoch 169/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.1492 - accuracy: 0.9422\n",
            "Epoch 00169: val_loss did not improve from 0.21938\n",
            "33280/33280 [==============================] - 10s 308us/sample - loss: 0.1492 - accuracy: 0.9422 - val_loss: 0.2212 - val_accuracy: 0.9121 - lr: 1.5625e-05\n",
            "Epoch 170/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.1480 - accuracy: 0.9444\n",
            "Epoch 00170: val_loss did not improve from 0.21938\n",
            "33280/33280 [==============================] - 10s 308us/sample - loss: 0.1480 - accuracy: 0.9444 - val_loss: 0.2210 - val_accuracy: 0.9114 - lr: 1.5625e-05\n",
            "Epoch 171/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.1490 - accuracy: 0.9450\n",
            "Epoch 00171: val_loss did not improve from 0.21938\n",
            "33280/33280 [==============================] - 10s 308us/sample - loss: 0.1490 - accuracy: 0.9450 - val_loss: 0.2228 - val_accuracy: 0.9097 - lr: 1.5625e-05\n",
            "Epoch 172/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.1491 - accuracy: 0.9437\n",
            "Epoch 00172: val_loss did not improve from 0.21938\n",
            "33280/33280 [==============================] - 10s 308us/sample - loss: 0.1491 - accuracy: 0.9437 - val_loss: 0.2198 - val_accuracy: 0.9109 - lr: 1.5625e-05\n",
            "Epoch 173/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.1505 - accuracy: 0.9438\n",
            "Epoch 00173: val_loss did not improve from 0.21938\n",
            "33280/33280 [==============================] - 10s 308us/sample - loss: 0.1505 - accuracy: 0.9438 - val_loss: 0.2211 - val_accuracy: 0.9109 - lr: 1.5625e-05\n",
            "Epoch 174/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.1524 - accuracy: 0.9423\n",
            "Epoch 00174: val_loss did not improve from 0.21938\n",
            "33280/33280 [==============================] - 10s 308us/sample - loss: 0.1524 - accuracy: 0.9423 - val_loss: 0.2202 - val_accuracy: 0.9114 - lr: 1.5625e-05\n",
            "Epoch 175/400\n",
            "33280/33280 [==============================] - ETA: 0s - loss: 0.1508 - accuracy: 0.9433\n",
            "Epoch 00175: val_loss did not improve from 0.21938\n",
            "\n",
            "Epoch 00175: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "33280/33280 [==============================] - 10s 309us/sample - loss: 0.1508 - accuracy: 0.9433 - val_loss: 0.2206 - val_accuracy: 0.9106 - lr: 1.5625e-05\n",
            "Epoch 00175: early stopping\n"
          ]
        }
      ],
      "source": [
        "model.fit(X_train, y_train, X_val, y_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mXs5OSGBfier"
      },
      "outputs": [],
      "source": [
        "model = DeepConvNet(input_shape=(1,62,400), num_class=4, batch_size = 1024 ,log_path='/content/drive/MyDrive/Brainwave_hack/model/bandpass' , lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit_continue(X_train, y_train, X_val, y_val)"
      ],
      "metadata": {
        "id": "LJE-JPj1GRX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_9g7UiCinc5"
      },
      "source": [
        "# Prediction Band pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3LKD_60qipeA"
      },
      "outputs": [],
      "source": [
        "x_test_public = np.load(f\"/content/drive/MyDrive/Brain Powerrrrrrrr/test_data_no_bandpass.npy\")\n",
        "x_test_private = np.load(f\"/content/drive/MyDrive/Brain Powerrrrrrrr/final_test_data_no_bandpass.npy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-wCnV_9i8RX",
        "outputId": "3c81ecf1-87f4-4979-d0df-0b73c576d2d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(400, 62, 400)\n",
            "(400, 62, 400)\n"
          ]
        }
      ],
      "source": [
        "print(x_test_public.shape)\n",
        "print(x_test_private.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_test =  np.concatenate((x_test_public, x_test_private) , axis = 0)"
      ],
      "metadata": {
        "id": "B00Q_tIbSONT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HDG8mp69i-QH"
      },
      "outputs": [],
      "source": [
        "x_test = np.expand_dims(x_test, axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zHv-8bW3jAtX",
        "outputId": "7ed74bf2-e298-4138-b558-a8219b437acd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(800, 1, 62, 400)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "x_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YnjKgvB7ivY-"
      },
      "outputs": [],
      "source": [
        "model = DeepConvNet(input_shape=(1,62,400), num_class=4, batch_size = 1024 ,log_path='/content/logs')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gm1U6bgEjDWp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5483c599-194b-4a3f-b568-e4b0f00f782a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_3 (InputLayer)         [(None, 1, 62, 400)]      0         \n",
            "_________________________________________________________________\n",
            "conv2d_10 (Conv2D)           (None, 25, 62, 396)       150       \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 25, 1, 396)        38775     \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 25, 1, 396)        100       \n",
            "_________________________________________________________________\n",
            "activation_10 (Activation)   (None, 25, 1, 396)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2 (None, 25, 1, 198)        0         \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 25, 1, 198)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_12 (Conv2D)           (None, 50, 1, 194)        6300      \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 50, 1, 194)        200       \n",
            "_________________________________________________________________\n",
            "activation_11 (Activation)   (None, 50, 1, 194)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_9 (MaxPooling2 (None, 50, 1, 97)         0         \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 50, 1, 97)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_13 (Conv2D)           (None, 100, 1, 93)        25100     \n",
            "_________________________________________________________________\n",
            "batch_normalization_10 (Batc (None, 100, 1, 93)        400       \n",
            "_________________________________________________________________\n",
            "activation_12 (Activation)   (None, 100, 1, 93)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_10 (MaxPooling (None, 100, 1, 46)        0         \n",
            "_________________________________________________________________\n",
            "dropout_10 (Dropout)         (None, 100, 1, 46)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_14 (Conv2D)           (None, 200, 1, 42)        100200    \n",
            "_________________________________________________________________\n",
            "batch_normalization_11 (Batc (None, 200, 1, 42)        800       \n",
            "_________________________________________________________________\n",
            "activation_13 (Activation)   (None, 200, 1, 42)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_11 (MaxPooling (None, 200, 1, 21)        0         \n",
            "_________________________________________________________________\n",
            "dropout_11 (Dropout)         (None, 200, 1, 21)        0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 4200)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 4)                 16804     \n",
            "_________________________________________________________________\n",
            "activation_14 (Activation)   (None, 4)                 0         \n",
            "=================================================================\n",
            "Total params: 188,829\n",
            "Trainable params: 188,079\n",
            "Non-trainable params: 750\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8zvTu5y8jHIu",
        "outputId": "91919279-6784-460c-d49b-344a0e3fae8e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 2, 2, 1, 0, 2, 0, 3, 1, 1, 0, 3, 1, 0, 0, 3, 3, 1, 3, 0, 0, 0,\n",
              "       0, 1, 1, 1, 2, 3, 0, 2, 2, 0, 1, 0, 1, 0, 1, 3, 1, 0, 3, 0, 0, 2,\n",
              "       2, 1, 1, 3, 1, 2, 3, 3, 1, 1, 3, 0, 0, 0, 0, 0, 2, 0, 1, 1, 1, 1,\n",
              "       3, 1, 0, 1, 3, 2, 2, 1, 2, 0, 3, 2, 0, 0, 2, 2, 1, 1, 1, 1, 1, 0,\n",
              "       3, 3, 2, 1, 0, 3, 3, 2, 3, 3, 3, 1, 3, 3, 0, 1, 2, 3, 2, 1, 1, 3,\n",
              "       0, 2, 2, 2, 0, 2, 3, 1, 3, 2, 3, 2, 0, 2, 2, 3, 3, 0, 2, 2, 0, 3,\n",
              "       1, 2, 2, 2, 2, 0, 2, 1, 0, 1, 3, 2, 2, 1, 0, 2, 3, 0, 0, 1, 3, 1,\n",
              "       0, 1, 3, 0, 0, 1, 3, 2, 3, 3, 3, 3, 3, 0, 3, 1, 3, 0, 2, 2, 3, 2,\n",
              "       3, 0, 2, 0, 2, 0, 2, 3, 2, 3, 1, 0, 3, 2, 1, 2, 2, 2, 1, 1, 1, 0,\n",
              "       0, 1, 2, 2, 3, 1, 1, 2, 3, 1, 2, 3, 1, 2, 2, 2, 2, 3, 1, 2, 3, 2,\n",
              "       1, 2, 0, 2, 2, 0, 2, 0, 3, 2, 1, 3, 3, 3, 2, 3, 3, 3, 1, 1, 2, 3,\n",
              "       2, 2, 2, 2, 2, 0, 0, 2, 0, 3, 1, 3, 0, 1, 3, 1, 2, 2, 1, 1, 2, 1,\n",
              "       2, 1, 0, 3, 1, 3, 0, 2, 3, 3, 0, 1, 0, 1, 1, 2, 1, 0, 1, 0, 1, 2,\n",
              "       0, 1, 1, 1, 2, 3, 1, 3, 0, 0, 1, 0, 2, 3, 0, 1, 0, 0, 1, 3, 0, 1,\n",
              "       1, 3, 1, 0, 1, 1, 3, 0, 1, 2, 1, 2, 1, 3, 1, 0, 0, 1, 0, 0, 2, 3,\n",
              "       0, 1, 1, 2, 3, 3, 0, 1, 0, 3, 3, 1, 1, 2, 0, 3, 0, 2, 1, 3, 1, 0,\n",
              "       0, 1, 2, 1, 2, 3, 2, 0, 1, 1, 2, 1, 0, 0, 3, 3, 0, 2, 1, 2, 1, 2,\n",
              "       0, 0, 1, 3, 3, 1, 1, 0, 1, 3, 0, 2, 2, 1, 3, 0, 0, 2, 0, 1, 3, 0,\n",
              "       3, 3, 0, 0, 0, 2, 3, 1, 0, 3, 1, 2, 0, 1, 0, 0, 0, 1, 1, 2, 2, 0,\n",
              "       2, 1, 1, 1, 3, 1, 0, 0, 2, 2, 0, 3, 3, 0, 0, 0, 0, 0, 1, 2, 0, 0,\n",
              "       3, 0, 0, 3, 3, 0, 0, 3, 0, 2, 2, 2, 0, 0, 3, 0, 1, 0, 0, 0, 3, 0,\n",
              "       1, 0, 1, 0, 2, 0, 0, 1, 2, 2, 3, 1, 3, 0, 1, 2, 0, 0, 2, 2, 0, 0,\n",
              "       0, 0, 0, 0, 2, 2, 2, 1, 1, 3, 3, 2, 2, 2, 3, 3, 3, 2, 1, 1, 2, 3,\n",
              "       2, 1, 0, 2, 0, 3, 3, 2, 0, 2, 3, 0, 2, 2, 3, 2, 0, 2, 3, 2, 3, 1,\n",
              "       0, 2, 0, 2, 0, 3, 3, 2, 2, 1, 3, 1, 0, 1, 3, 3, 0, 3, 0, 2, 3, 1,\n",
              "       0, 0, 2, 0, 1, 1, 2, 0, 0, 0, 2, 2, 3, 3, 2, 2, 2, 2, 3, 0, 2, 1,\n",
              "       3, 3, 2, 2, 0, 1, 2, 0, 3, 1, 3, 3, 2, 2, 0, 1, 2, 3, 3, 2, 2, 2,\n",
              "       0, 0, 0, 0, 0, 1, 2, 2, 3, 1, 2, 2, 2, 1, 3, 2, 0, 2, 2, 3, 2, 2,\n",
              "       0, 3, 3, 3, 1, 2, 1, 2, 2, 0, 2, 1, 2, 3, 1, 2, 2, 3, 2, 2, 2, 2,\n",
              "       0, 0, 2, 2, 2, 1, 2, 2, 3, 1, 0, 2, 1, 2, 0, 3, 1, 0, 3, 0, 3, 3,\n",
              "       0, 1, 2, 0, 2, 0, 1, 3, 1, 2, 1, 2, 2, 2, 0, 0, 0, 0, 1, 2, 3, 0,\n",
              "       0, 1, 0, 3, 2, 0, 1, 0, 2, 2, 0, 3, 0, 1, 2, 0, 3, 2, 0, 0, 0, 1,\n",
              "       0, 3, 0, 0, 0, 2, 1, 0, 1, 1, 2, 0, 0, 2, 0, 3, 0, 2, 1, 1, 1, 1,\n",
              "       0, 1, 3, 2, 0, 0, 0, 2, 2, 2, 1, 1, 0, 2, 3, 0, 0, 2, 1, 3, 0, 3,\n",
              "       0, 3, 0, 3, 0, 1, 3, 1, 3, 2, 3, 0, 0, 0, 3, 0, 1, 0, 2, 2, 2, 3,\n",
              "       1, 3, 0, 2, 1, 1, 1, 2, 2, 0, 1, 0, 0, 3, 0, 3, 2, 3, 2, 1, 3, 2,\n",
              "       0, 1, 3, 0, 3, 2, 1, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "pred"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2_UUfdGTSX7",
        "outputId": "b19f1fc3-2d9e-429f-e745-e686252356eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 2, 0, 1, 0, 2, 1, 3, 3, 1, 0, 1, 1, 0, 0, 1, 3, 1, 3, 2, 0, 0,\n",
              "       2, 1, 2, 1, 3, 3, 0, 2, 2, 0, 1, 0, 1, 0, 1, 2, 1, 0, 3, 0, 0, 3,\n",
              "       2, 1, 1, 3, 1, 2, 3, 2, 1, 1, 3, 1, 0, 0, 0, 0, 2, 0, 1, 0, 1, 1,\n",
              "       3, 1, 0, 1, 3, 1, 2, 1, 3, 0, 1, 2, 0, 0, 3, 2, 1, 1, 1, 1, 0, 0,\n",
              "       3, 3, 3, 1, 1, 3, 3, 0, 3, 3, 2, 3, 3, 2, 0, 3, 2, 2, 2, 1, 1, 1,\n",
              "       0, 2, 2, 2, 0, 2, 3, 1, 3, 2, 3, 0, 0, 2, 2, 3, 3, 0, 2, 2, 0, 1,\n",
              "       0, 3, 2, 2, 2, 0, 2, 1, 0, 1, 3, 2, 0, 3, 0, 2, 3, 1, 1, 1, 3, 1,\n",
              "       1, 1, 2, 0, 0, 1, 3, 2, 1, 3, 3, 2, 3, 2, 3, 1, 3, 0, 0, 3, 3, 2,\n",
              "       3, 2, 2, 0, 2, 0, 2, 3, 3, 3, 1, 0, 3, 2, 2, 2, 2, 2, 1, 1, 1, 0,\n",
              "       0, 1, 2, 2, 3, 1, 2, 3, 3, 1, 2, 3, 1, 2, 2, 2, 2, 3, 1, 2, 3, 2,\n",
              "       1, 2, 2, 2, 2, 0, 2, 0, 3, 2, 1, 3, 3, 3, 2, 3, 2, 3, 0, 3, 2, 3,\n",
              "       3, 0, 2, 3, 2, 0, 1, 2, 0, 3, 1, 2, 0, 1, 3, 1, 2, 2, 1, 1, 2, 2,\n",
              "       2, 1, 0, 3, 1, 3, 0, 3, 2, 3, 0, 1, 3, 1, 1, 0, 1, 0, 1, 2, 2, 2,\n",
              "       0, 1, 1, 1, 2, 3, 1, 3, 0, 1, 1, 0, 3, 3, 0, 1, 1, 0, 1, 1, 3, 1,\n",
              "       1, 3, 1, 2, 3, 1, 3, 0, 1, 2, 1, 3, 1, 3, 1, 0, 0, 1, 0, 0, 2, 2,\n",
              "       0, 1, 1, 0, 3, 3, 2, 1, 0, 2, 3, 1, 1, 2, 0, 3, 0, 2, 1, 1, 1, 0,\n",
              "       0, 1, 2, 1, 2, 3, 2, 0, 0, 1, 3, 1, 0, 1, 3, 3, 2, 2, 1, 3, 1, 0,\n",
              "       0, 0, 1, 3, 3, 0, 1, 0, 1, 1, 0, 2, 2, 3, 3, 0, 2, 2, 0, 1, 3, 0,\n",
              "       3, 3, 0, 0, 0, 2, 2, 0, 0, 2, 0, 2, 0, 1, 0, 0, 0, 1, 1, 2, 2, 0,\n",
              "       2, 1, 1, 0, 2, 1, 0, 0, 2, 2, 0, 3, 3, 0, 0, 0, 0, 0, 1, 2, 0, 0,\n",
              "       2, 0, 0, 3, 3, 0, 0, 3, 0, 3, 2, 2, 0, 0, 3, 0, 1, 0, 0, 0, 3, 0,\n",
              "       0, 0, 1, 0, 2, 0, 0, 0, 2, 2, 3, 0, 3, 0, 0, 2, 0, 0, 2, 2, 0, 0,\n",
              "       1, 0, 0, 0, 2, 2, 2, 0, 0, 3, 3, 2, 2, 2, 3, 3, 3, 2, 1, 0, 2, 2,\n",
              "       2, 0, 0, 2, 0, 3, 2, 2, 0, 3, 3, 0, 2, 2, 2, 2, 0, 2, 3, 2, 3, 0,\n",
              "       0, 2, 1, 2, 0, 3, 3, 2, 2, 0, 2, 0, 1, 0, 2, 2, 0, 2, 0, 2, 3, 0,\n",
              "       0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 2, 3, 3, 2, 2, 2, 2, 2, 3, 1, 2, 1,\n",
              "       2, 2, 2, 2, 0, 0, 2, 0, 3, 1, 3, 2, 2, 2, 0, 0, 2, 3, 3, 3, 2, 0,\n",
              "       0, 0, 0, 0, 0, 0, 2, 3, 0, 0, 3, 3, 3, 0, 3, 2, 0, 0, 2, 3, 2, 2,\n",
              "       0, 0, 3, 2, 0, 0, 2, 2, 2, 0, 3, 1, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2,\n",
              "       0, 0, 2, 2, 2, 1, 2, 2, 3, 1, 0, 2, 1, 2, 0, 2, 0, 0, 2, 0, 3, 2,\n",
              "       0, 1, 2, 0, 2, 0, 1, 3, 1, 2, 1, 2, 2, 2, 0, 0, 0, 0, 1, 2, 2, 1,\n",
              "       0, 1, 0, 3, 0, 0, 1, 0, 2, 2, 0, 3, 0, 1, 2, 0, 2, 2, 0, 0, 0, 1,\n",
              "       0, 2, 0, 0, 0, 2, 1, 0, 0, 0, 2, 0, 0, 2, 0, 2, 0, 2, 0, 1, 0, 1,\n",
              "       0, 1, 2, 2, 0, 0, 0, 2, 0, 2, 0, 1, 1, 2, 3, 0, 0, 2, 1, 3, 0, 2,\n",
              "       0, 2, 0, 3, 0, 1, 3, 1, 3, 2, 3, 0, 0, 0, 3, 0, 1, 0, 2, 2, 2, 3,\n",
              "       1, 2, 0, 2, 0, 1, 1, 3, 2, 1, 1, 1, 1, 0, 1, 3, 2, 3, 2, 1, 3, 2,\n",
              "       0, 0, 3, 1, 3, 2, 1, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wADrroAsjLG2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "submission = pd.read_csv('/content/sample_submission (1).csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VRzp8aRHjRVI",
        "outputId": "6dc8a7c8-a0c9-4d22-c683-cea6624dc94d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        }
      ],
      "source": [
        "submission['prediction'][:] = pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RhM_2NGYjTVe"
      },
      "outputs": [],
      "source": [
        "submission = submission.fillna(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xjb8kWyvjVCP"
      },
      "outputs": [],
      "source": [
        "submission['prediction'] = submission['prediction'].astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "SANmEjjAjXHd",
        "outputId": "3c75f902-f5dc-4c36-9aa9-e9b81c6e9eac"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    sample_id  prediction\n",
              "0           1           0\n",
              "1           2           2\n",
              "2           3           2\n",
              "3           4           1\n",
              "4           5           0\n",
              "..        ...         ...\n",
              "795      f396           0\n",
              "796      f397           3\n",
              "797      f398           2\n",
              "798      f399           1\n",
              "799      f400           0\n",
              "\n",
              "[800 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-57e1509d-1fbb-4d2d-8fb8-7f13338da5ec\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sample_id</th>\n",
              "      <th>prediction</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>795</th>\n",
              "      <td>f396</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>796</th>\n",
              "      <td>f397</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>797</th>\n",
              "      <td>f398</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>798</th>\n",
              "      <td>f399</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>799</th>\n",
              "      <td>f400</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>800 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-57e1509d-1fbb-4d2d-8fb8-7f13338da5ec')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-57e1509d-1fbb-4d2d-8fb8-7f13338da5ec button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-57e1509d-1fbb-4d2d-8fb8-7f13338da5ec');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "submission"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "submission"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "t0st0ciHTtVE",
        "outputId": "f7d200b9-15ce-4727-c865-8bb37b2844c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    sample_id  prediction\n",
              "0           1           0\n",
              "1           2           2\n",
              "2           3           2\n",
              "3           4           1\n",
              "4           5           0\n",
              "..        ...         ...\n",
              "795      f396           0\n",
              "796      f397           3\n",
              "797      f398           2\n",
              "798      f399           1\n",
              "799      f400           0\n",
              "\n",
              "[800 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a233e8af-3c0f-4e52-8efd-8c57244ab979\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sample_id</th>\n",
              "      <th>prediction</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>795</th>\n",
              "      <td>f396</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>796</th>\n",
              "      <td>f397</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>797</th>\n",
              "      <td>f398</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>798</th>\n",
              "      <td>f399</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>799</th>\n",
              "      <td>f400</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>800 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a233e8af-3c0f-4e52-8efd-8c57244ab979')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a233e8af-3c0f-4e52-8efd-8c57244ab979 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a233e8af-3c0f-4e52-8efd-8c57244ab979');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CHjDt0FnjaPH"
      },
      "outputs": [],
      "source": [
        "submission.to_csv('/content/not_working.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prediction normal"
      ],
      "metadata": {
        "id": "VIiW-qIiZ0vx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_test = np.load(f\"/content/drive/MyDrive/Brainwave_hack/data/100_final_test.npy\")"
      ],
      "metadata": {
        "id": "xzWW6vFsZ3br"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_test = np.expand_dims(x_test, axis = 1)"
      ],
      "metadata": {
        "id": "CikhVL-JaIkS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VDdkU9VCaGEf",
        "outputId": "5487d1d2-8979-4243-cc03-4862869aa5f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(800, 1, 62, 400)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = DeepConvNet(input_shape=(1,62,400), num_class=4, batch_size = 1024 ,log_path='/content/drive/MyDrive/Brainwave_hack/model/holyshit20val')"
      ],
      "metadata": {
        "id": "1G85YVywaSrR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred = model.predict(x_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e11Lh_6saXak",
        "outputId": "6adea68a-c9fd-4095-91c3-0aa72a80b891"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 1, 62, 400)]      0         \n",
            "_________________________________________________________________\n",
            "conv2d (Conv2D)              (None, 25, 62, 396)       150       \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 25, 1, 396)        38775     \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 25, 1, 396)        100       \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 25, 1, 396)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 25, 1, 198)        0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 25, 1, 198)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 50, 1, 194)        6300      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 50, 1, 194)        200       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 50, 1, 194)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 50, 1, 97)         0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 50, 1, 97)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 100, 1, 93)        25100     \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 100, 1, 93)        400       \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 100, 1, 93)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 100, 1, 46)        0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 100, 1, 46)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 200, 1, 42)        100200    \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 200, 1, 42)        800       \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 200, 1, 42)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 200, 1, 21)        0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 200, 1, 21)        0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 4200)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 4)                 16804     \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 4)                 0         \n",
            "=================================================================\n",
            "Total params: 188,829\n",
            "Trainable params: 188,079\n",
            "Non-trainable params: 750\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZTVxI2Cbaa8q",
        "outputId": "c86ca10f-2093-4998-f5bc-d65d34489f6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 2, 3, 1, 0, 2, 0, 3, 1, 1, 0, 3, 1, 0, 0, 3, 3, 1, 3, 0, 0, 0,\n",
              "       1, 1, 1, 1, 3, 3, 0, 2, 2, 0, 1, 0, 1, 0, 1, 3, 1, 0, 3, 1, 0, 3,\n",
              "       2, 1, 1, 3, 1, 2, 3, 3, 1, 1, 3, 0, 0, 0, 0, 0, 2, 0, 1, 0, 1, 1,\n",
              "       3, 1, 0, 1, 3, 1, 2, 1, 2, 0, 3, 2, 0, 0, 3, 2, 1, 1, 1, 1, 1, 0,\n",
              "       3, 3, 3, 1, 0, 3, 3, 2, 3, 3, 3, 3, 3, 3, 0, 1, 2, 3, 2, 1, 1, 3,\n",
              "       0, 2, 2, 3, 0, 2, 3, 0, 3, 2, 3, 3, 0, 2, 2, 3, 3, 0, 3, 2, 0, 1,\n",
              "       1, 2, 2, 2, 2, 0, 2, 1, 0, 1, 3, 2, 0, 3, 1, 3, 3, 0, 0, 1, 3, 1,\n",
              "       1, 1, 3, 0, 0, 1, 3, 2, 3, 3, 3, 3, 3, 1, 2, 1, 3, 0, 2, 3, 3, 2,\n",
              "       3, 0, 2, 0, 2, 0, 2, 3, 3, 3, 1, 0, 3, 2, 2, 2, 2, 2, 1, 1, 1, 0,\n",
              "       0, 0, 2, 2, 3, 1, 2, 3, 3, 1, 2, 3, 1, 2, 2, 2, 2, 3, 1, 2, 3, 3,\n",
              "       1, 2, 0, 2, 3, 0, 3, 0, 3, 3, 1, 3, 3, 3, 2, 3, 3, 3, 1, 0, 2, 3,\n",
              "       3, 2, 2, 2, 2, 0, 1, 2, 0, 3, 1, 2, 0, 1, 3, 1, 2, 2, 1, 1, 2, 1,\n",
              "       2, 1, 0, 3, 1, 3, 0, 1, 2, 3, 0, 1, 0, 1, 1, 2, 3, 0, 1, 0, 0, 2,\n",
              "       0, 1, 1, 1, 2, 3, 1, 3, 0, 1, 2, 1, 2, 3, 1, 1, 1, 0, 1, 3, 0, 1,\n",
              "       1, 2, 1, 0, 1, 1, 3, 0, 1, 2, 1, 2, 1, 3, 0, 0, 0, 1, 0, 0, 2, 3,\n",
              "       0, 1, 1, 2, 3, 3, 1, 1, 0, 1, 3, 1, 1, 2, 0, 3, 0, 2, 1, 3, 1, 0,\n",
              "       1, 1, 2, 1, 2, 3, 2, 0, 1, 1, 3, 1, 0, 1, 3, 3, 0, 2, 1, 2, 1, 2,\n",
              "       0, 0, 1, 3, 3, 0, 1, 0, 1, 3, 0, 2, 2, 2, 3, 0, 0, 2, 0, 1, 3, 0,\n",
              "       3, 3, 0, 1, 0, 2, 2, 1, 0, 3, 1, 2, 0, 1, 0, 0, 0, 0, 1, 2, 2, 0,\n",
              "       2, 1, 1, 1, 3, 0, 0, 0, 2, 2, 0, 3, 3, 0, 0, 0, 0, 0, 1, 2, 0, 1,\n",
              "       3, 0, 0, 3, 3, 0, 0, 3, 0, 2, 2, 2, 0, 0, 3, 0, 1, 0, 0, 0, 2, 0,\n",
              "       1, 0, 1, 0, 2, 0, 0, 1, 2, 2, 3, 1, 2, 0, 1, 2, 0, 0, 2, 2, 0, 0,\n",
              "       0, 0, 0, 0, 2, 2, 3, 0, 1, 3, 3, 2, 2, 2, 3, 3, 3, 2, 1, 0, 2, 2,\n",
              "       2, 0, 0, 2, 0, 3, 3, 2, 0, 2, 3, 0, 2, 2, 3, 2, 0, 2, 3, 2, 3, 0,\n",
              "       0, 2, 0, 2, 0, 3, 3, 2, 2, 1, 3, 1, 0, 1, 2, 3, 0, 3, 0, 2, 3, 1,\n",
              "       0, 0, 2, 0, 1, 1, 2, 0, 0, 0, 2, 2, 3, 3, 2, 2, 2, 2, 3, 0, 2, 1,\n",
              "       3, 2, 2, 2, 0, 1, 2, 0, 3, 1, 3, 3, 2, 2, 0, 0, 2, 3, 2, 3, 2, 2,\n",
              "       0, 0, 0, 0, 0, 1, 2, 2, 3, 1, 3, 2, 2, 1, 3, 2, 0, 2, 2, 3, 2, 2,\n",
              "       0, 3, 2, 2, 1, 2, 0, 2, 2, 0, 2, 1, 2, 2, 1, 2, 2, 3, 2, 2, 2, 2,\n",
              "       0, 0, 2, 2, 2, 1, 2, 2, 2, 1, 0, 2, 1, 3, 0, 3, 0, 0, 3, 0, 3, 2,\n",
              "       0, 1, 2, 1, 2, 0, 1, 3, 1, 2, 1, 2, 2, 2, 0, 0, 0, 0, 1, 2, 3, 1,\n",
              "       0, 1, 0, 3, 2, 0, 1, 0, 2, 2, 0, 2, 0, 0, 3, 0, 3, 2, 0, 0, 0, 0,\n",
              "       0, 3, 0, 0, 0, 2, 1, 0, 1, 0, 2, 0, 1, 2, 0, 3, 0, 2, 0, 1, 0, 1,\n",
              "       0, 1, 3, 2, 0, 0, 1, 2, 2, 2, 1, 1, 0, 2, 3, 0, 0, 2, 1, 3, 0, 3,\n",
              "       0, 2, 0, 3, 0, 1, 3, 1, 3, 2, 2, 0, 0, 0, 3, 0, 1, 0, 2, 2, 2, 3,\n",
              "       1, 3, 0, 2, 1, 1, 1, 2, 2, 0, 0, 0, 0, 3, 0, 3, 2, 3, 2, 1, 3, 2,\n",
              "       0, 0, 3, 0, 3, 2, 1, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "submission = pd.read_csv('/content/sample_submission.csv')"
      ],
      "metadata": {
        "id": "wFMXt9xradbI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission['prediction'][:] = pred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VuxhLYXYahcR",
        "outputId": "ab029a37-faa4-4bdc-f7b2-7144a117cbb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "submission['prediction'] = submission['prediction'].astype(int)"
      ],
      "metadata": {
        "id": "1l70Yc9tanvk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "oZO4N8n4azQd",
        "outputId": "abeb998b-1c23-46f6-bc36-94da8c64c56a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    sample_id  prediction\n",
              "0           1           0\n",
              "1           2           2\n",
              "2           3           3\n",
              "3           4           1\n",
              "4           5           0\n",
              "..        ...         ...\n",
              "795      f396           0\n",
              "796      f397           3\n",
              "797      f398           2\n",
              "798      f399           1\n",
              "799      f400           0\n",
              "\n",
              "[800 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d1124dfa-f07f-44ca-8baa-da25370ca5a1\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sample_id</th>\n",
              "      <th>prediction</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>795</th>\n",
              "      <td>f396</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>796</th>\n",
              "      <td>f397</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>797</th>\n",
              "      <td>f398</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>798</th>\n",
              "      <td>f399</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>799</th>\n",
              "      <td>f400</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>800 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d1124dfa-f07f-44ca-8baa-da25370ca5a1')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d1124dfa-f07f-44ca-8baa-da25370ca5a1 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d1124dfa-f07f-44ca-8baa-da25370ca5a1');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "submission.to_csv('/content/maximum_power.csv', index=False)"
      ],
      "metadata": {
        "id": "Bo_-W4G5a2RU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MIN2NET"
      ],
      "metadata": {
        "id": "Tmi_8pR9LEWw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install min2net"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bAeezQnS6pSN",
        "outputId": "24107a78-991f-43c2-8383-f5eaf919a7f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: min2net in /usr/local/lib/python3.7/dist-packages (1.0.1)\n",
            "Requirement already satisfied: wheel>=0.37.0 in /usr/local/lib/python3.7/dist-packages (from min2net) (0.37.1)\n",
            "Requirement already satisfied: scikit-learn>=0.24.1 in /usr/local/lib/python3.7/dist-packages (from min2net) (1.0.2)\n",
            "Requirement already satisfied: setuptools>=42 in /usr/local/lib/python3.7/dist-packages (from min2net) (57.4.0)\n",
            "Requirement already satisfied: wget>=3.2 in /usr/local/lib/python3.7/dist-packages (from min2net) (3.2)\n",
            "Requirement already satisfied: tensorflow-addons==0.9.1 in /usr/local/lib/python3.7/dist-packages (from min2net) (0.9.1)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons==0.9.1->min2net) (2.7.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.24.1->min2net) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.24.1->min2net) (1.19.5)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.24.1->min2net) (1.4.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.24.1->min2net) (3.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.19.5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KtRgxmrI9KR0",
        "outputId": "4c50b551-f72d-450e-fd0e-56dc7dc13944"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy==1.19.5 in /usr/local/lib/python3.7/dist-packages (1.19.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Concatenate, AveragePooling2D, BatchNormalization, Conv2D, Conv2DTranspose, Dense, Input, Reshape, Flatten\n",
        "from tensorflow.keras.constraints import max_norm\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import CSVLogger, ModelCheckpoint, ReduceLROnPlateau\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "from min2net.loss import mean_squared_error, triplet_loss, SparseCategoricalCrossentropy\n",
        "from min2net.utils import TimeHistory, compute_class_weight\n",
        "\n",
        "class MIN2Net:\n",
        "    def __init__(self,\n",
        "                input_shape=(1,400,20), \n",
        "                num_class=2, \n",
        "                loss=[triplet_loss(margin=1.0), 'sparse_categorical_crossentropy'],\n",
        "                loss_weights=[1., 1.], \n",
        "                latent_dim = None,\n",
        "                epochs=200,\n",
        "                batch_size=100,\n",
        "                optimizer=Adam(beta_1=0.9, beta_2=0.999, epsilon=1e-08),\n",
        "                lr=1e-2,\n",
        "                min_lr=1e-3,\n",
        "                factor=0.5,\n",
        "                patience=5, \n",
        "                es_patience=20,\n",
        "                verbose=1,\n",
        "                log_path='logs',\n",
        "                model_name='MIN2Net', \n",
        "                **kwargs):\n",
        "        D, T, C = input_shape\n",
        "        self.latent_dim = latent_dim if latent_dim is not None else C if num_class==2 else 64\n",
        "        self.num_class = num_class\n",
        "        self.input_shape = input_shape\n",
        "        self.loss = loss\n",
        "        self.loss_weights = loss_weights\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.optimizer = optimizer\n",
        "        self.optimizer.lr = lr\n",
        "        self.lr = lr\n",
        "        self.min_lr = min_lr\n",
        "        self.factor = factor\n",
        "        self.patience = patience\n",
        "        self.es_patience = es_patience\n",
        "        self.verbose = verbose\n",
        "        self.log_path = log_path\n",
        "        self.model_name = model_name\n",
        "        self.weights_dir = log_path+'/'+model_name+'_out_weights.h5'\n",
        "        self.csv_dir = log_path+'/'+model_name+'_out_log.log'\n",
        "        self.time_log = log_path+'/'+model_name+'_time_log.csv'\n",
        "\n",
        "        # use **kwargs to set the new value of below args.\n",
        "        self.f1_average = 'binary' if self.num_class == 2 else 'macro'\n",
        "        self.data_format = 'channels_last'\n",
        "        self.shuffle = False\n",
        "        self.metrics = 'accuracy'\n",
        "        self.monitor = 'val_loss'\n",
        "        self.mode = 'min'\n",
        "        self.save_best_only = True\n",
        "        self.save_weight_only = True\n",
        "        self.seed = 1234\n",
        "        self.class_balancing = False\n",
        "        # 'set params'\n",
        "        self.subsampling_size = 100\n",
        "        self.pool_size_1 = (1,T//self.subsampling_size)\n",
        "        self.pool_size_2 = (1,4)\n",
        "        self.filter_1 = C\n",
        "        self.filter_2 = 10\n",
        "        \n",
        "        for k in kwargs.keys():\n",
        "            self.__setattr__(k, kwargs[k])\n",
        "        \n",
        "        self.flatten_size = T//self.pool_size_1[1]//self.pool_size_2[1]\n",
        "        \n",
        "        np.random.seed(self.seed)\n",
        "        tf.random.set_seed(self.seed)\n",
        "        K.set_image_data_format(self.data_format)\n",
        "        if not os.path.exists(self.log_path):\n",
        "            os.makedirs(self.log_path)\n",
        "\n",
        "    def build(self):\n",
        "        'encoder'\n",
        "        encoder_input  = Input(self.input_shape)\n",
        "        en_conv        = Conv2D(self.filter_1, (1, 64), activation='elu', padding=\"same\", \n",
        "                                kernel_constraint=max_norm(2., axis=(0, 1, 2)))(encoder_input)\n",
        "        en_conv        = BatchNormalization(axis=3, epsilon=1e-05, momentum=0.1)(en_conv)\n",
        "        en_conv        = AveragePooling2D(pool_size=self.pool_size_1)(en_conv)  \n",
        "        en_conv        = Conv2D(self.filter_2, (1, 32), activation='elu', padding=\"same\", \n",
        "                                kernel_constraint=max_norm(2., axis=(0, 1, 2)))(en_conv)\n",
        "        en_conv        = BatchNormalization(axis=3, epsilon=1e-05, momentum=0.1)(en_conv)\n",
        "        en_conv        = AveragePooling2D(pool_size=self.pool_size_2)(en_conv)\n",
        "        en_conv        = Flatten()(en_conv)\n",
        "        encoder_output = Dense(self.latent_dim, kernel_constraint=max_norm(0.5))(en_conv)\n",
        "        encoder        = Model(inputs=encoder_input, outputs=encoder_output, name='encoder')\n",
        "        encoder.summary()\n",
        "        \n",
        "        # 'decoder'\n",
        "        # decoder_input  = Input(shape=(self.latent_dim,), name='decoder_input')\n",
        "        # de_conv        = Dense(1*self.flatten_size*self.filter_2, activation='elu', \n",
        "        #                        kernel_constraint=max_norm(0.5))(decoder_input)\n",
        "        # de_conv        = Reshape((1, self.flatten_size, self.filter_2))(de_conv)\n",
        "        # de_conv        = Conv2DTranspose(filters=self.filter_2, kernel_size=(1, 64), \n",
        "        #                                  activation='elu', padding='same', strides=self.pool_size_2, \n",
        "        #                                  kernel_constraint=max_norm(2., axis=(0, 1, 2)))(de_conv)\n",
        "        # decoder_output = Conv2DTranspose(filters=self.filter_1, kernel_size=(1, 32), \n",
        "        #                                  activation='elu', padding='same', strides=self.pool_size_1, \n",
        "        #                                  kernel_constraint=max_norm(2., axis=(0, 1, 2)))(de_conv)\n",
        "        # decoder        = Model(inputs=decoder_input, outputs=decoder_output, name='decoder')\n",
        "        # decoder.summary()\n",
        "\n",
        "        'Build the computation graph for training'\n",
        "        latent         = encoder(encoder_input)\n",
        "        # train_xr       = decoder(latent)\n",
        "        z              = Dense(self.num_class, activation='softmax', kernel_constraint=max_norm(0.5), \n",
        "                               name='classifier')(latent)\n",
        "\n",
        "        return Model(inputs=encoder_input, outputs=[latent, z], \n",
        "                            name='MIN2Net')\n",
        "    \n",
        "    def fit(self, X_train, y_train, X_val, y_val):\n",
        "            \n",
        "        if X_train.ndim != 4:\n",
        "            raise Exception('ValueError: `X_train` is incompatible: expected ndim=4, found ndim='+str(X_train.ndim))\n",
        "        elif X_val.ndim != 4:\n",
        "            raise Exception('ValueError: `X_val` is incompatible: expected ndim=4, found ndim='+str(X_val.ndim))\n",
        "\n",
        "        csv_logger    = CSVLogger(self.csv_dir)\n",
        "        time_callback = TimeHistory(self.time_log)\n",
        "        checkpointer  = ModelCheckpoint(monitor=self.monitor, filepath=self.weights_dir, \n",
        "                                        verbose=self.verbose, save_best_only=self.save_best_only, \n",
        "                                        save_weight_only=self.save_weight_only)\n",
        "        reduce_lr     = ReduceLROnPlateau(monitor=self.monitor, patience=self.patience, \n",
        "                                          factor=self.factor, mode=self.mode, verbose=self.verbose, \n",
        "                                          min_lr=self.min_lr)\n",
        "        es            = EarlyStopping(monitor=self.monitor, mode=self.mode, verbose=self.verbose, \n",
        "                                      patience=self.es_patience)\n",
        "        model = self.build()     \n",
        "        model.summary()\n",
        "        \n",
        "        if self.class_balancing: # compute_class_weight if class_balancing is True\n",
        "            class_weight  = compute_class_weight(y_train)\n",
        "            self.loss[-1] = SparseCategoricalCrossentropy(class_weight=class_weight)\n",
        "        \n",
        "        model.compile(optimizer=self.optimizer, loss=self.loss, metrics=[self.metrics], loss_weights=self.loss_weights)\n",
        "\n",
        "        model.fit(x=X_train, y=[X_train,y_train,y_train],\n",
        "                          batch_size=self.batch_size, shuffle=self.shuffle,\n",
        "                          epochs=self.epochs, validation_data=(X_val, [X_val,y_val,y_val]),\n",
        "                          callbacks=[checkpointer,csv_logger,reduce_lr,es])\n",
        "        \n",
        "    def predict(self, X_test):\n",
        "\n",
        "        if X_test.ndim != 4:\n",
        "            raise Exception('ValueError: `X_test` is incompatible: expected ndim=4, found ndim='+str(X_test.ndim))\n",
        "\n",
        "        model = self.build()\n",
        "        model.summary()\n",
        "        model.load_weights(self.weights_dir)\n",
        "        model.compile(optimizer=self.optimizer, loss=self.loss, metrics=[self.metrics], loss_weights=self.loss_weights)\n",
        "\n",
        "        start = time.time()\n",
        "        y_pred_decoder, y_pred_trip, y_pred_clf = model.predict(X_test)\n",
        "        end = time.time()\n",
        "\n",
        "        y_pred_argm = np.argmax(y_pred_clf, axis=1)\n",
        "\n",
        "        return y_pred_argm"
      ],
      "metadata": {
        "id": "sM00ZidILGDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "min2net=MIN2Net(input_shape=(1,400,62), \n",
        "                num_class=4, \n",
        "                loss=[mean_squared_error, triplet_loss(margin=1.0), 'sparse_categorical_crossentropy'],\n",
        "                loss_weights=[1., 2.], \n",
        "                latent_dim = None,\n",
        "                epochs=400,\n",
        "                batch_size=1024,\n",
        "                optimizer=Adam(beta_1=0.9, beta_2=0.999, epsilon=1e-08),\n",
        "                lr=1e-3,\n",
        "                min_lr=1e-5,\n",
        "                factor=0.5,\n",
        "                patience=5, \n",
        "                es_patience=20)"
      ],
      "metadata": {
        "id": "7ybpEwk77AW5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "session_x_merge =  np.float32(np.concatenate((np.load(f\"/content/drive/MyDrive/Brain Powerrrrrrrr/offline_data_no_bandpass.npy\"), np.load(f\"/content/drive/MyDrive/Brain Powerrrrrrrr/online_data_no_bandpass.npy\")) , axis = 0))\n",
        "session_y_merge = np.concatenate((np.load(f\"/content/drive/MyDrive/Brain Powerrrrrrrr/offline_label_no_bandpass.npy\"), np.load(f\"/content/drive/MyDrive/Brain Powerrrrrrrr/online_label_no_bandpass.npy\")) , axis = 0)"
      ],
      "metadata": {
        "id": "mONMBM0m7eu5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "session_x_merge.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xUdvKPOJ7mNQ",
        "outputId": "d6696251-2ff2-48af-8a4b-1aa5512e4c16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(41600, 62, 400)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.swapaxes(session_x_merge, 1, 2).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OM1eSdCe8KFO",
        "outputId": "58947531-f73f-4948-fab0-efe0b12fab23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(41600, 400, 62)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(np.swapaxes(session_x_merge, 1, 2)[0,:,0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "ao5TXzMf8P_Z",
        "outputId": "cdf0159a-8fac-4d6d-c569-ecd13ebb5656"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f5927031750>]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "session_x_merge=np.expand_dims(np.swapaxes(session_x_merge, 1, 2),1)\n",
        "session_x_merge.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9JSoc0x7jSv",
        "outputId": "d19a541d-a62a-4972-8c19-3fdd58219558"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(41600, 1, 400, 62)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(session_x_merge, session_y_merge, test_size=0.2, random_state=44)\n",
        "print(\"Number of posters for training: \", len(X_train))\n",
        "print(\"Number of posters for validation: \", len(X_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vMa14EB88hiK",
        "outputId": "364714b8-0117-4ce0-a9cf-d27b68a12e2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of posters for training:  33280\n",
            "Number of posters for validation:  8320\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del session_x_merge\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NTQ8YX8X9oJ8",
        "outputId": "9beb76b5-1c36-4401-8b4f-ba3dbed052d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "161"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "min2net.fit(X_train,y_train,X_val,y_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "VMTIIvo98k25",
        "outputId": "820e7a05-8b6d-46b8-907d-caf51217acf9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/resource_variable_ops.py:1666: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "Model: \"encoder\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 1, 400, 62)]      0         \n",
            "_________________________________________________________________\n",
            "conv2d (Conv2D)              (None, 1, 400, 62)        246078    \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 1, 400, 62)        248       \n",
            "_________________________________________________________________\n",
            "average_pooling2d (AveragePo (None, 1, 100, 62)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 1, 100, 10)        19850     \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 1, 100, 10)        40        \n",
            "_________________________________________________________________\n",
            "average_pooling2d_1 (Average (None, 1, 25, 10)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 250)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 64)                16064     \n",
            "=================================================================\n",
            "Total params: 282,280\n",
            "Trainable params: 282,136\n",
            "Non-trainable params: 144\n",
            "_________________________________________________________________\n",
            "Model: \"decoder\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "decoder_input (InputLayer)   [(None, 64)]              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 250)               16250     \n",
            "_________________________________________________________________\n",
            "reshape (Reshape)            (None, 1, 25, 10)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose (Conv2DTran (None, 1, 100, 10)        6410      \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_1 (Conv2DTr (None, 1, 400, 62)        19902     \n",
            "=================================================================\n",
            "Total params: 42,562\n",
            "Trainable params: 42,562\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"MIN2Net\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 1, 400, 62)] 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "encoder (Model)                 (None, 64)           282280      input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "decoder (Model)                 (None, 1, 400, 62)   42562       encoder[1][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "classifier (Dense)              (None, 4)            260         encoder[1][0]                    \n",
            "==================================================================================================\n",
            "Total params: 325,102\n",
            "Trainable params: 324,958\n",
            "Non-trainable params: 144\n",
            "__________________________________________________________________________________________________\n",
            "Train on 33280 samples, validate on 8320 samples\n",
            "Epoch 1/400\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ResourceExhaustedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-95d13b7a9d14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmin2net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-2-43a17d69594b>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X_train, y_train, X_val, y_val)\u001b[0m\n\u001b[1;32m    153\u001b[0m                           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m                           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m                           callbacks=[checkpointer,csv_logger,reduce_lr,es])\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training_v1.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    783\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    784\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 785\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    787\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    664\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m         \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 666\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    667\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    387\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3630\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3631\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3632\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3633\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3634\u001b[0m     output_structure = nest.pack_sequence_as(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mResourceExhaustedError\u001b[0m: 2 root error(s) found.\n  (0) Resource exhausted: OOM when allocating tensor with shape[1048576,1024] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node Cast}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[loss/Identity_1/_465]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n  (1) Resource exhausted: OOM when allocating tensor with shape[1048576,1024] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node Cast}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n0 successful operations.\n0 derived errors ignored."
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Deep learning Brain.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}